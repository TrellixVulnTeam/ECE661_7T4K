{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (a) Model preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from resnet20 import ResNetCIFAR\n",
    "from train_util import train, finetune, test\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from FP_layers import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = ResNetCIFAR(num_layers=20, Nbits=None)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3231, Test accuracy=0.9151\n"
     ]
    }
   ],
   "source": [
    "# Load the best weight paramters\n",
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (b) Prune by percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_by_percentage(layer, q=80.0):\n",
    "    \"\"\"\n",
    "    Pruning the weight paramters by threshold.\n",
    "    :param q: pruning percentile. 'q' percent of the least \n",
    "    significant weight parameters will be pruned.\n",
    "    \"\"\"\n",
    "    # Convert the weight of \"layer\" to numpy array\n",
    "    out = layer.weight\n",
    "    weight = out.cpu().detach().numpy()\n",
    "    size=weight.shape\n",
    "    weight_re = weight.reshape(-1,1)\n",
    "    \n",
    "    \n",
    "    # Compute the q-th percentile of the abs of the converted array\n",
    "    weight_abs = np.abs(weight_re)\n",
    "    N = weight_abs.shape[0]\n",
    "    index = np.argsort(weight_abs,axis=0)\n",
    "    N_drop = int(np.ceil(N*(q*0.01)))\n",
    "    index_drop = index[0:N_drop]\n",
    "    \n",
    "    # Generate a binary mask same shape as weight to decide which element to prune\n",
    "    mask = np.ones((N,1))\n",
    "    mask[index_drop] = 0\n",
    "    mask =mask.reshape(size)\n",
    "\n",
    "    #weight_new = np.multiply(mask, weight_re)\n",
    "    #weight_output = weight_new.reshape(size)\n",
    "    \n",
    "    # Convert mask to torch tensor and put on GPU\n",
    "    mask = torch.from_numpy(mask).to(layer.weight.device)\n",
    "\n",
    "    \n",
    "    # Multiply the weight by mask to perform pruning\n",
    "    output = torch.mul(mask, out)\n",
    "    layer.weight.data = output.float()\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of head_conv.0.conv: 0.40046296296296297\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.4001736111111111\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.4001736111111111\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.4001736111111111\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.4001736111111111\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.4001736111111111\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.4001736111111111\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.4001736111111111\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.4000651041666667\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.4000651041666667\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.4000651041666667\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.4000651041666667\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.4000651041666667\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.4000108506944444\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.4000108506944444\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.4000108506944444\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.4000108506944444\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.4000108506944444\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.4000108506944444\n",
      "Sparsity of final_fc.linear: 0.4\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.4304, Test accuracy=0.8871\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        # change q value\n",
    "        prune_by_percentage(layer, q=40.0)\n",
    "        \n",
    "        # Optional: Check the sparsity you achieve in each layer\n",
    "        # Convert the weight of \"layer\" to numpy array\n",
    "        out = layer.weight\n",
    "        weight = out.cpu().detach().numpy()\n",
    "        weight_re = weight.reshape(-1,1)\n",
    "        \n",
    "        weight_abs = np.abs(weight_re)\n",
    "        N = weight_abs.shape[0]\n",
    "        \n",
    "        #np_weight = \n",
    "        # Count number of zeros\n",
    "        zeros = N- np.count_nonzero(weight_abs)\n",
    "        #zeros = \n",
    "        # Count number of parameters\n",
    "        #total = \n",
    "        total = weight_abs.shape[0]\n",
    "        # Print sparsity\n",
    "        print('Sparsity of '+name+': '+str(zeros/total))\n",
    "        \n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of head_conv.0.conv: 0.40046296296296297\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.4001736111111111\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.4001736111111111\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.4001736111111111\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.4001736111111111\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.4001736111111111\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.4001736111111111\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.4001736111111111\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.4000651041666667\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.4000651041666667\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.4000651041666667\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.4000651041666667\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.4000651041666667\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.4000108506944444\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.4000108506944444\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.4000108506944444\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.4000108506944444\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.4000108506944444\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.4000108506944444\n",
      "Sparsity of final_fc.linear: 0.4\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.4304, Test accuracy=0.8871\n",
      "Sparsity of head_conv.0.conv: 0.6018518518518519\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.6002604166666666\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.6002604166666666\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.6002604166666666\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.6002604166666666\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.6002604166666666\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.6002604166666666\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.6000434027777778\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.6000434027777778\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.6000434027777778\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.6000434027777778\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.6000434027777778\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.6000434027777778\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.6000434027777778\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.6000162760416666\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.6000162760416666\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.6000162760416666\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.6000162760416666\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.6000162760416666\n",
      "Sparsity of final_fc.linear: 0.6\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ff9a178e6446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Print sparsity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sparsity of '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/hw4/train_util.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(net)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for item in [40,60,80]:\n",
    "    net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "\n",
    "    for name,layer in net.named_modules():\n",
    "        if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "            # change q value\n",
    "            prune_by_percentage(layer, q=item)\n",
    "\n",
    "            # Optional: Check the sparsity you achieve in each layer\n",
    "            # Convert the weight of \"layer\" to numpy array\n",
    "            out = layer.weight\n",
    "            weight = out.cpu().detach().numpy()\n",
    "            weight_re = weight.reshape(-1,1)\n",
    "\n",
    "            weight_abs = np.abs(weight_re)\n",
    "            N = weight_abs.shape[0]\n",
    "\n",
    "            #np_weight = \n",
    "            # Count number of zeros\n",
    "            zeros = N- np.count_nonzero(weight_abs)\n",
    "            #zeros = \n",
    "            # Count number of parameters\n",
    "            #total = \n",
    "            total = weight_abs.shape[0]\n",
    "            # Print sparsity\n",
    "            print('Sparsity of '+name+': '+str(zeros/total))\n",
    "    test(net)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (c) Finetune pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_after_prune(net, trainloader, criterion, optimizer, prune=True):\n",
    "    \"\"\"\n",
    "    Finetune the pruned model for a single epoch\n",
    "    Make sure pruned weights are kept as zero\n",
    "    \"\"\"\n",
    "    # Build a dictionary for the nonzero weights\n",
    "    weight_mask = {}\n",
    "    for name,layer in net.named_modules():\n",
    "        if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "            # Your code here: generate a mask in GPU torch tensor to have 1 for nonzero element and 0 for zero element \n",
    "            out = layer.weight\n",
    "            weight = out.clone()\n",
    "            weight[weight!=0] = 1      \n",
    "            weight_mask[name] = weight\n",
    "    \n",
    "    global_steps = 0\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if prune:\n",
    "            for name,layer in net.named_modules():\n",
    "                if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "                    # Your code here: Use weight_mask to make sure zero elements remains zero\n",
    "                    weight_id_mask = weight_mask[name]\n",
    "                    weight_id_layer=layer.weight\n",
    "                    layer.weight.data = torch.mul(weight_id_mask ,weight_id_layer)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        global_steps += 1\n",
    "\n",
    "        if global_steps % 50 == 0:\n",
    "            end = time.time()\n",
    "            num_examples_per_second = 50 * batch_size / (end - start)\n",
    "            print(\"[Step=%d]\\tLoss=%.4f\\tacc=%.4f\\t%.1f examples/second\"\n",
    "                 % (global_steps, train_loss / (batch_idx + 1), (correct / total), num_examples_per_second))\n",
    "            start = time.time()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Get pruned model\n",
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        prune_by_percentage(layer, q=80.0)\n",
    "\n",
    "# Training setup, do not change\n",
    "batch_size=256\n",
    "lr=0.002\n",
    "reg=1e-4\n",
    "\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.875, weight_decay=reg, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.9567\tacc=0.6908\t2569.5 examples/second\n",
      "[Step=100]\tLoss=0.7980\tacc=0.7398\t2986.1 examples/second\n",
      "[Step=150]\tLoss=0.7123\tacc=0.7645\t3195.0 examples/second\n",
      "Test Loss=0.5566, Test acc=0.8184\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=50]\tLoss=0.4621\tacc=0.8436\t3465.2 examples/second\n",
      "[Step=100]\tLoss=0.4559\tacc=0.8439\t3003.9 examples/second\n",
      "[Step=150]\tLoss=0.4506\tacc=0.8454\t2968.8 examples/second\n",
      "Test Loss=0.4842, Test acc=0.8402\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=50]\tLoss=0.4012\tacc=0.8621\t3590.0 examples/second\n",
      "[Step=100]\tLoss=0.3959\tacc=0.8646\t3654.9 examples/second\n",
      "[Step=150]\tLoss=0.3922\tacc=0.8666\t3297.8 examples/second\n",
      "Test Loss=0.4540, Test acc=0.8508\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=50]\tLoss=0.3661\tacc=0.8745\t3229.8 examples/second\n",
      "[Step=100]\tLoss=0.3677\tacc=0.8758\t3722.0 examples/second\n",
      "[Step=150]\tLoss=0.3621\tacc=0.8774\t3225.1 examples/second\n",
      "Test Loss=0.4344, Test acc=0.8585\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=50]\tLoss=0.3523\tacc=0.8811\t3395.3 examples/second\n",
      "[Step=100]\tLoss=0.3426\tacc=0.8849\t3495.5 examples/second\n",
      "[Step=150]\tLoss=0.3422\tacc=0.8840\t3257.1 examples/second\n",
      "Test Loss=0.4214, Test acc=0.8624\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=50]\tLoss=0.3277\tacc=0.8875\t3299.5 examples/second\n",
      "[Step=100]\tLoss=0.3293\tacc=0.8881\t3830.2 examples/second\n",
      "[Step=150]\tLoss=0.3281\tacc=0.8881\t3335.1 examples/second\n",
      "Test Loss=0.4129, Test acc=0.8633\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=50]\tLoss=0.3192\tacc=0.8893\t3119.8 examples/second\n",
      "[Step=100]\tLoss=0.3162\tacc=0.8906\t3326.8 examples/second\n",
      "[Step=150]\tLoss=0.3138\tacc=0.8917\t3253.0 examples/second\n",
      "Test Loss=0.4051, Test acc=0.8655\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=50]\tLoss=0.3000\tacc=0.9002\t3429.6 examples/second\n",
      "[Step=100]\tLoss=0.3014\tacc=0.8989\t4050.8 examples/second\n",
      "[Step=150]\tLoss=0.3028\tacc=0.8986\t3296.0 examples/second\n",
      "Test Loss=0.3987, Test acc=0.8663\n",
      "Saving...\n",
      "\n",
      "Epoch: 8\n",
      "[Step=50]\tLoss=0.3074\tacc=0.8919\t3428.1 examples/second\n",
      "[Step=100]\tLoss=0.2985\tacc=0.8958\t3676.3 examples/second\n",
      "[Step=150]\tLoss=0.2965\tacc=0.8973\t3398.6 examples/second\n",
      "Test Loss=0.3936, Test acc=0.8678\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=50]\tLoss=0.2913\tacc=0.8979\t2687.7 examples/second\n",
      "[Step=100]\tLoss=0.2911\tacc=0.8994\t3256.1 examples/second\n",
      "[Step=150]\tLoss=0.2876\tacc=0.9004\t3195.8 examples/second\n",
      "Test Loss=0.3893, Test acc=0.8690\n",
      "Saving...\n",
      "\n",
      "Epoch: 10\n",
      "[Step=50]\tLoss=0.2841\tacc=0.9020\t3473.3 examples/second\n",
      "[Step=100]\tLoss=0.2833\tacc=0.9025\t3333.3 examples/second\n",
      "[Step=150]\tLoss=0.2816\tacc=0.9033\t3234.9 examples/second\n",
      "Test Loss=0.3860, Test acc=0.8717\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=50]\tLoss=0.2805\tacc=0.9033\t2893.1 examples/second\n",
      "[Step=100]\tLoss=0.2771\tacc=0.9054\t3807.7 examples/second\n",
      "[Step=150]\tLoss=0.2743\tacc=0.9059\t3331.9 examples/second\n",
      "Test Loss=0.3812, Test acc=0.8739\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=50]\tLoss=0.2796\tacc=0.9015\t3281.7 examples/second\n",
      "[Step=100]\tLoss=0.2797\tacc=0.9026\t3528.7 examples/second\n",
      "[Step=150]\tLoss=0.2781\tacc=0.9039\t2927.8 examples/second\n",
      "Test Loss=0.3791, Test acc=0.8736\n",
      "\n",
      "Epoch: 13\n",
      "[Step=50]\tLoss=0.2754\tacc=0.9087\t3386.9 examples/second\n",
      "[Step=100]\tLoss=0.2693\tacc=0.9068\t3579.6 examples/second\n",
      "[Step=150]\tLoss=0.2670\tacc=0.9080\t3177.5 examples/second\n",
      "Test Loss=0.3777, Test acc=0.8739\n",
      "\n",
      "Epoch: 14\n",
      "[Step=50]\tLoss=0.2625\tacc=0.9098\t3183.5 examples/second\n",
      "[Step=100]\tLoss=0.2617\tacc=0.9097\t3563.5 examples/second\n",
      "[Step=150]\tLoss=0.2606\tacc=0.9103\t3433.3 examples/second\n",
      "Test Loss=0.3744, Test acc=0.8750\n",
      "Saving...\n",
      "\n",
      "Epoch: 15\n",
      "[Step=50]\tLoss=0.2554\tacc=0.9101\t2859.1 examples/second\n",
      "[Step=100]\tLoss=0.2554\tacc=0.9126\t3645.3 examples/second\n",
      "[Step=150]\tLoss=0.2581\tacc=0.9103\t3326.2 examples/second\n",
      "Test Loss=0.3737, Test acc=0.8755\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=50]\tLoss=0.2577\tacc=0.9098\t3902.4 examples/second\n",
      "[Step=100]\tLoss=0.2535\tacc=0.9122\t3976.6 examples/second\n",
      "[Step=150]\tLoss=0.2538\tacc=0.9118\t3809.2 examples/second\n",
      "Test Loss=0.3702, Test acc=0.8779\n",
      "Saving...\n",
      "\n",
      "Epoch: 17\n",
      "[Step=50]\tLoss=0.2486\tacc=0.9158\t3066.9 examples/second\n",
      "[Step=100]\tLoss=0.2486\tacc=0.9146\t3125.4 examples/second\n",
      "[Step=150]\tLoss=0.2485\tacc=0.9152\t2988.0 examples/second\n",
      "Test Loss=0.3703, Test acc=0.8772\n",
      "\n",
      "Epoch: 18\n",
      "[Step=50]\tLoss=0.2440\tacc=0.9157\t3518.3 examples/second\n",
      "[Step=100]\tLoss=0.2473\tacc=0.9148\t3397.1 examples/second\n",
      "[Step=150]\tLoss=0.2469\tacc=0.9144\t3463.5 examples/second\n",
      "Test Loss=0.3675, Test acc=0.8788\n",
      "Saving...\n",
      "\n",
      "Epoch: 19\n",
      "[Step=50]\tLoss=0.2436\tacc=0.9163\t3013.0 examples/second\n",
      "[Step=100]\tLoss=0.2439\tacc=0.9156\t3546.8 examples/second\n",
      "[Step=150]\tLoss=0.2431\tacc=0.9163\t3398.0 examples/second\n",
      "Test Loss=0.3656, Test acc=0.8781\n"
     ]
    }
   ],
   "source": [
    "# Model finetuning\n",
    "for epoch in range(20):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    finetune_after_prune(net, trainloader, criterion, optimizer)\n",
    "    #Start the testing code.\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    num_val_steps = len(testloader)\n",
    "    val_acc = correct / total\n",
    "    print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        print(\"Saving...\")\n",
    "        torch.save(net.state_dict(), \"net_after_finetune.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sparsity of the finetuned model, make sure it's not changed\n",
    "net.load_state_dict(torch.load(\"net_after_finetune.pt\"))\n",
    "\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        # Your code here:\n",
    "        # Convert the weight of \"layer\" to numpy array\n",
    "        out = layer.weight\n",
    "        weight = out.cpu().detach().numpy()\n",
    "        weight_re = weight.reshape(-1,1)\n",
    "\n",
    "        weight_abs = np.abs(weight_re)\n",
    "        N = weight_abs.shape[0]\n",
    "\n",
    "        # Count number of zeros\n",
    "        zeros = N- np.count_nonzero(weight_abs)\n",
    "        # Count number of parameters\n",
    "        total = weight_abs.shape[0]\n",
    "        # Print sparsity\n",
    "        print('Sparsity of '+name+': '+str(zeros/total))\n",
    "        \n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (d) Iterative pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.0470\tacc=0.9857\t3439.6 examples/second\n",
      "[Step=100]\tLoss=0.0469\tacc=0.9861\t3785.6 examples/second\n",
      "[Step=150]\tLoss=0.0471\tacc=0.9856\t3610.0 examples/second\n",
      "Test Loss=0.3237, Test acc=0.9147\n",
      "\n",
      "Epoch: 1\n",
      "[Step=50]\tLoss=0.0531\tacc=0.9821\t3574.8 examples/second\n",
      "[Step=100]\tLoss=0.0509\tacc=0.9839\t3663.5 examples/second\n",
      "[Step=150]\tLoss=0.0502\tacc=0.9841\t3629.7 examples/second\n",
      "Test Loss=0.3241, Test acc=0.9151\n",
      "\n",
      "Epoch: 2\n",
      "[Step=50]\tLoss=0.0542\tacc=0.9819\t3483.7 examples/second\n",
      "[Step=100]\tLoss=0.0526\tacc=0.9827\t3521.0 examples/second\n",
      "[Step=150]\tLoss=0.0510\tacc=0.9835\t3647.3 examples/second\n",
      "Test Loss=0.3287, Test acc=0.9133\n",
      "\n",
      "Epoch: 3\n",
      "[Step=50]\tLoss=0.0610\tacc=0.9788\t3126.9 examples/second\n",
      "[Step=100]\tLoss=0.0586\tacc=0.9804\t3726.5 examples/second\n",
      "[Step=150]\tLoss=0.0580\tacc=0.9804\t3692.8 examples/second\n",
      "Test Loss=0.3347, Test acc=0.9121\n",
      "\n",
      "Epoch: 4\n",
      "[Step=50]\tLoss=0.0717\tacc=0.9759\t3417.9 examples/second\n",
      "[Step=100]\tLoss=0.0707\tacc=0.9761\t3460.9 examples/second\n",
      "[Step=150]\tLoss=0.0689\tacc=0.9767\t3851.7 examples/second\n",
      "Test Loss=0.3368, Test acc=0.9087\n",
      "\n",
      "Epoch: 5\n",
      "[Step=50]\tLoss=0.0879\tacc=0.9694\t3539.9 examples/second\n",
      "[Step=100]\tLoss=0.0888\tacc=0.9695\t4049.5 examples/second\n",
      "[Step=150]\tLoss=0.0880\tacc=0.9693\t3568.7 examples/second\n",
      "Test Loss=0.3361, Test acc=0.9048\n",
      "\n",
      "Epoch: 6\n",
      "[Step=50]\tLoss=0.1299\tacc=0.9541\t3443.4 examples/second\n",
      "[Step=100]\tLoss=0.1240\tacc=0.9569\t3481.4 examples/second\n",
      "[Step=150]\tLoss=0.1200\tacc=0.9580\t3828.5 examples/second\n",
      "Test Loss=0.3362, Test acc=0.9006\n",
      "\n",
      "Epoch: 7\n",
      "[Step=50]\tLoss=0.1889\tacc=0.9349\t3421.2 examples/second\n",
      "[Step=100]\tLoss=0.1771\tacc=0.9371\t4025.6 examples/second\n",
      "[Step=150]\tLoss=0.1698\tacc=0.9399\t4016.6 examples/second\n",
      "Test Loss=0.3510, Test acc=0.8927\n",
      "\n",
      "Epoch: 8\n",
      "[Step=50]\tLoss=0.2764\tacc=0.9028\t3417.4 examples/second\n",
      "[Step=100]\tLoss=0.2641\tacc=0.9069\t3827.9 examples/second\n",
      "[Step=150]\tLoss=0.2504\tacc=0.9114\t3551.0 examples/second\n",
      "Test Loss=0.3711, Test acc=0.8819\n",
      "\n",
      "Epoch: 9\n",
      "[Step=50]\tLoss=0.5559\tacc=0.8106\t3140.1 examples/second\n",
      "[Step=100]\tLoss=0.5061\tacc=0.8257\t3736.6 examples/second\n",
      "[Step=150]\tLoss=0.4777\tacc=0.8354\t3405.5 examples/second\n",
      "Test Loss=0.4788, Test acc=0.8439\n",
      "\n",
      "Epoch: 10\n",
      "[Step=50]\tLoss=0.3811\tacc=0.8691\t3031.6 examples/second\n",
      "[Step=100]\tLoss=0.3789\tacc=0.8712\t3657.8 examples/second\n",
      "[Step=150]\tLoss=0.3745\tacc=0.8728\t3281.4 examples/second\n",
      "Test Loss=0.4440, Test acc=0.8529\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=50]\tLoss=0.3419\tacc=0.8841\t3456.2 examples/second\n",
      "[Step=100]\tLoss=0.3471\tacc=0.8822\t3542.7 examples/second\n",
      "[Step=150]\tLoss=0.3450\tacc=0.8824\t3625.9 examples/second\n",
      "Test Loss=0.4244, Test acc=0.8593\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=50]\tLoss=0.3154\tacc=0.8946\t3170.7 examples/second\n",
      "[Step=100]\tLoss=0.3201\tacc=0.8923\t3609.4 examples/second\n",
      "[Step=150]\tLoss=0.3176\tacc=0.8929\t3605.2 examples/second\n",
      "Test Loss=0.4109, Test acc=0.8633\n",
      "Saving...\n",
      "\n",
      "Epoch: 13\n",
      "[Step=50]\tLoss=0.3064\tacc=0.8934\t2919.1 examples/second\n",
      "[Step=100]\tLoss=0.3056\tacc=0.8937\t3561.7 examples/second\n",
      "[Step=150]\tLoss=0.3019\tacc=0.8965\t3431.9 examples/second\n",
      "Test Loss=0.4030, Test acc=0.8666\n",
      "Saving...\n",
      "\n",
      "Epoch: 14\n",
      "[Step=50]\tLoss=0.3109\tacc=0.8930\t3463.5 examples/second\n",
      "[Step=100]\tLoss=0.3030\tacc=0.8948\t3631.4 examples/second\n",
      "[Step=150]\tLoss=0.3020\tacc=0.8952\t3936.1 examples/second\n",
      "Test Loss=0.3962, Test acc=0.8671\n",
      "Saving...\n",
      "\n",
      "Epoch: 15\n",
      "[Step=50]\tLoss=0.2803\tacc=0.9035\t3421.1 examples/second\n",
      "[Step=100]\tLoss=0.2842\tacc=0.9020\t4160.2 examples/second\n",
      "[Step=150]\tLoss=0.2828\tacc=0.9029\t4116.3 examples/second\n",
      "Test Loss=0.3921, Test acc=0.8705\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=50]\tLoss=0.2831\tacc=0.9012\t3207.6 examples/second\n",
      "[Step=100]\tLoss=0.2774\tacc=0.9046\t3532.3 examples/second\n",
      "[Step=150]\tLoss=0.2767\tacc=0.9048\t3423.5 examples/second\n",
      "Test Loss=0.3861, Test acc=0.8724\n",
      "Saving...\n",
      "\n",
      "Epoch: 17\n",
      "[Step=50]\tLoss=0.2747\tacc=0.9045\t3505.2 examples/second\n",
      "[Step=100]\tLoss=0.2718\tacc=0.9060\t3951.6 examples/second\n",
      "[Step=150]\tLoss=0.2725\tacc=0.9055\t3343.4 examples/second\n",
      "Test Loss=0.3843, Test acc=0.8734\n",
      "Saving...\n",
      "\n",
      "Epoch: 18\n",
      "[Step=50]\tLoss=0.2581\tacc=0.9138\t3255.1 examples/second\n",
      "[Step=100]\tLoss=0.2609\tacc=0.9114\t3771.8 examples/second\n",
      "[Step=150]\tLoss=0.2630\tacc=0.9097\t3773.8 examples/second\n",
      "Test Loss=0.3800, Test acc=0.8747\n",
      "Saving...\n",
      "\n",
      "Epoch: 19\n",
      "[Step=50]\tLoss=0.2631\tacc=0.9101\t3304.4 examples/second\n",
      "[Step=100]\tLoss=0.2594\tacc=0.9111\t3276.4 examples/second\n",
      "[Step=150]\tLoss=0.2591\tacc=0.9114\t3373.5 examples/second\n",
      "Test Loss=0.3772, Test acc=0.8754\n",
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "best_acc = 0.\n",
    "for epoch in range(20):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    \n",
    "    net.train()\n",
    "    if epoch<10:\n",
    "        for name,layer in net.named_modules():\n",
    "            if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "                # Increase model sparsity\n",
    "                q = 8 *(epoch+1)\n",
    "                prune_by_percentage(layer, q=q)\n",
    "    if epoch<9:\n",
    "        finetune_after_prune(net, trainloader, criterion, optimizer,prune=False)\n",
    "    else:\n",
    "        finetune_after_prune(net, trainloader, criterion, optimizer)\n",
    "    \n",
    "    #Start the testing code.\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    num_val_steps = len(testloader)\n",
    "    val_acc = correct / total\n",
    "    print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n",
    "    \n",
    "    if epoch>=10:\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(\"Saving...\")\n",
    "            torch.save(net.state_dict(), \"net_after_iterative_prune.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of head_conv.0.conv: 0.8009259259259259\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.8003472222222222\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.8003472222222222\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.8003472222222222\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.8003472222222222\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.8003472222222222\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.8003472222222222\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.8001302083333334\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.8000217013888888\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.8000217013888888\n",
      "Sparsity of final_fc.linear: 0.8\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3772, Test accuracy=0.8754\n"
     ]
    }
   ],
   "source": [
    "# Check sparsity of the final model, make sure it's 80%\n",
    "net.load_state_dict(torch.load(\"net_after_iterative_prune.pt\"))\n",
    "\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        # Your code here: can copy from previous question\n",
    "        out = layer.weight\n",
    "        weight = out.cpu().detach().numpy()\n",
    "        weight_re = weight.reshape(-1,1)\n",
    "\n",
    "        weight_abs = np.abs(weight_re)\n",
    "        N = weight_abs.shape[0]\n",
    "\n",
    "        # Count number of zeros\n",
    "        zeros = N- np.count_nonzero(weight_abs)\n",
    "        # Count number of parameters\n",
    "        total = weight_abs.shape[0]\n",
    "        # Print sparsity\n",
    "        print('Sparsity of '+name+': '+str(zeros/total))\n",
    "        \n",
    "        \n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (e) Global iterative pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_prune_by_percentage(net, q=80.0):\n",
    "    \"\"\"\n",
    "    Pruning the weight paramters by threshold.\n",
    "    :param q: pruning percentile. 'q' percent of the least \n",
    "    significant weight parameters will be pruned.\n",
    "    \"\"\"\n",
    "    # A list to gather all the weights\n",
    "    flattened_weights = []\n",
    "    # Find global pruning threshold\n",
    "    for name,layer in net.named_modules():\n",
    "        if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "            # Convert weight to numpy\n",
    "            out = layer.weight\n",
    "            weight = out.cpu().detach().numpy()\n",
    "            size=weight.shape\n",
    "            weight_re = np.abs(weight).reshape(-1,1)\n",
    "            \n",
    "            flattened_weights.append(weight_re)\n",
    "            \n",
    "            # Flatten the weight and append to flattened_weights\n",
    "    \n",
    "    # Concate all weights into a np array\n",
    "    \n",
    "    \n",
    "    flattened_weights= np.concatenate(flattened_weights)\n",
    "    # Find global pruning threshold\n",
    "\n",
    "    thres = np.percentile(flattened_weights,q)\n",
    "    print(thres)\n",
    "\n",
    "    \n",
    "    # Apply pruning threshold to all layers\n",
    "    for name,layer in net.named_modules():\n",
    "        if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "            # Convert weight to numpy\n",
    "            out = layer.weight\n",
    "            weight = out.cpu().detach().numpy()\n",
    "            weight_abs = np.abs(weight)\n",
    "            size=weight_abs.shape\n",
    "            \n",
    "            # Generate a binary mask same shape as weight to decide which element to prune\n",
    "            mask = np.where(weight_abs<thres,0,1)\n",
    "       \n",
    "            # Convert mask to torch tensor and put on GPU            \n",
    "            mask = torch.from_numpy(mask).to(layer.weight.device)\n",
    "\n",
    "    \n",
    "            # Multiply the weight by mask to perform pruning\n",
    "            output = torch.mul(mask, out)\n",
    "            layer.weight.data = output#.float()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0.0085436649620533\n",
      "[Step=50]\tLoss=0.0502\tacc=0.9848\t2732.0 examples/second\n",
      "[Step=100]\tLoss=0.0490\tacc=0.9851\t3805.0 examples/second\n",
      "[Step=150]\tLoss=0.0480\tacc=0.9848\t3532.1 examples/second\n",
      "Test Loss=0.3246, Test acc=0.9143\n",
      "\n",
      "Epoch: 1\n",
      "0.01718289256095886\n",
      "[Step=50]\tLoss=0.0487\tacc=0.9846\t3008.5 examples/second\n",
      "[Step=100]\tLoss=0.0483\tacc=0.9847\t3893.2 examples/second\n",
      "[Step=150]\tLoss=0.0493\tacc=0.9839\t3513.1 examples/second\n",
      "Test Loss=0.3235, Test acc=0.9157\n",
      "\n",
      "Epoch: 2\n",
      "0.026158433780074118\n",
      "[Step=50]\tLoss=0.0514\tacc=0.9826\t3465.8 examples/second\n",
      "[Step=100]\tLoss=0.0502\tacc=0.9834\t3779.5 examples/second\n",
      "[Step=150]\tLoss=0.0501\tacc=0.9834\t3833.5 examples/second\n",
      "Test Loss=0.3250, Test acc=0.9152\n",
      "\n",
      "Epoch: 3\n",
      "0.03560339510440826\n",
      "[Step=50]\tLoss=0.0536\tacc=0.9824\t3448.3 examples/second\n",
      "[Step=100]\tLoss=0.0546\tacc=0.9820\t3336.2 examples/second\n",
      "[Step=150]\tLoss=0.0539\tacc=0.9822\t3160.9 examples/second\n",
      "Test Loss=0.3251, Test acc=0.9128\n",
      "\n",
      "Epoch: 4\n",
      "0.04540476202964783\n",
      "[Step=50]\tLoss=0.0594\tacc=0.9802\t3113.3 examples/second\n",
      "[Step=100]\tLoss=0.0603\tacc=0.9801\t3661.3 examples/second\n",
      "[Step=150]\tLoss=0.0611\tacc=0.9797\t3252.0 examples/second\n",
      "Test Loss=0.3209, Test acc=0.9130\n",
      "\n",
      "Epoch: 5\n",
      "0.056169121712446216\n",
      "[Step=50]\tLoss=0.0696\tacc=0.9760\t3268.9 examples/second\n",
      "[Step=100]\tLoss=0.0679\tacc=0.9771\t3854.6 examples/second\n",
      "[Step=150]\tLoss=0.0688\tacc=0.9768\t3381.9 examples/second\n",
      "Test Loss=0.3251, Test acc=0.9076\n",
      "\n",
      "Epoch: 6\n",
      "0.06834546178579332\n",
      "[Step=50]\tLoss=0.0936\tacc=0.9677\t2770.7 examples/second\n",
      "[Step=100]\tLoss=0.0922\tacc=0.9680\t3063.6 examples/second\n",
      "[Step=150]\tLoss=0.0911\tacc=0.9688\t3194.2 examples/second\n",
      "Test Loss=0.3261, Test acc=0.9060\n",
      "\n",
      "Epoch: 7\n",
      "0.08207650482654572\n",
      "[Step=50]\tLoss=0.1294\tacc=0.9566\t3390.2 examples/second\n",
      "[Step=100]\tLoss=0.1259\tacc=0.9571\t4035.5 examples/second\n",
      "[Step=150]\tLoss=0.1240\tacc=0.9579\t3332.5 examples/second\n",
      "Test Loss=0.3253, Test acc=0.9024\n",
      "\n",
      "Epoch: 8\n",
      "0.09840235263109207\n",
      "[Step=50]\tLoss=0.1999\tacc=0.9320\t3432.1 examples/second\n",
      "[Step=100]\tLoss=0.1959\tacc=0.9322\t3468.2 examples/second\n",
      "[Step=150]\tLoss=0.1864\tacc=0.9353\t3252.6 examples/second\n",
      "Test Loss=0.3378, Test acc=0.8873\n",
      "\n",
      "Epoch: 9\n",
      "0.11939702928066254\n",
      "[Step=50]\tLoss=0.4071\tacc=0.8618\t3446.6 examples/second\n",
      "[Step=100]\tLoss=0.3838\tacc=0.8692\t3523.9 examples/second\n",
      "[Step=150]\tLoss=0.3696\tacc=0.8728\t3175.0 examples/second\n",
      "Test Loss=0.4214, Test acc=0.8582\n",
      "\n",
      "Epoch: 10\n",
      "[Step=50]\tLoss=0.3154\tacc=0.8929\t3378.0 examples/second\n",
      "[Step=100]\tLoss=0.3129\tacc=0.8940\t3471.9 examples/second\n",
      "[Step=150]\tLoss=0.3099\tacc=0.8953\t3272.0 examples/second\n",
      "Test Loss=0.3955, Test acc=0.8648\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=50]\tLoss=0.2810\tacc=0.9074\t3309.1 examples/second\n",
      "[Step=100]\tLoss=0.2830\tacc=0.9064\t3715.7 examples/second\n",
      "[Step=150]\tLoss=0.2856\tacc=0.9045\t3213.4 examples/second\n",
      "Test Loss=0.3832, Test acc=0.8692\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=50]\tLoss=0.2665\tacc=0.9085\t2621.2 examples/second\n",
      "[Step=100]\tLoss=0.2718\tacc=0.9077\t3024.5 examples/second\n",
      "[Step=150]\tLoss=0.2694\tacc=0.9077\t3100.7 examples/second\n",
      "Test Loss=0.3758, Test acc=0.8720\n",
      "Saving...\n",
      "\n",
      "Epoch: 13\n",
      "[Step=50]\tLoss=0.2658\tacc=0.9099\t3039.3 examples/second\n",
      "[Step=100]\tLoss=0.2610\tacc=0.9117\t3497.8 examples/second\n",
      "[Step=150]\tLoss=0.2607\tacc=0.9114\t3684.3 examples/second\n",
      "Test Loss=0.3678, Test acc=0.8751\n",
      "Saving...\n",
      "\n",
      "Epoch: 14\n",
      "[Step=50]\tLoss=0.2517\tacc=0.9155\t3158.7 examples/second\n",
      "[Step=100]\tLoss=0.2524\tacc=0.9149\t3076.0 examples/second\n",
      "[Step=150]\tLoss=0.2534\tacc=0.9145\t3253.7 examples/second\n",
      "Test Loss=0.3621, Test acc=0.8775\n",
      "Saving...\n",
      "\n",
      "Epoch: 15\n",
      "[Step=50]\tLoss=0.2514\tacc=0.9164\t3004.7 examples/second\n",
      "[Step=100]\tLoss=0.2496\tacc=0.9166\t3261.8 examples/second\n",
      "[Step=150]\tLoss=0.2465\tacc=0.9165\t3330.8 examples/second\n",
      "Test Loss=0.3593, Test acc=0.8792\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=50]\tLoss=0.2416\tacc=0.9196\t3539.9 examples/second\n",
      "[Step=100]\tLoss=0.2396\tacc=0.9199\t3595.5 examples/second\n",
      "[Step=150]\tLoss=0.2382\tacc=0.9201\t3394.4 examples/second\n",
      "Test Loss=0.3532, Test acc=0.8802\n",
      "Saving...\n",
      "\n",
      "Epoch: 17\n",
      "[Step=50]\tLoss=0.2311\tacc=0.9205\t3358.0 examples/second\n",
      "[Step=100]\tLoss=0.2302\tacc=0.9211\t2943.8 examples/second\n",
      "[Step=150]\tLoss=0.2315\tacc=0.9209\t3123.8 examples/second\n",
      "Test Loss=0.3541, Test acc=0.8798\n",
      "\n",
      "Epoch: 18\n",
      "[Step=50]\tLoss=0.2266\tacc=0.9227\t3419.5 examples/second\n",
      "[Step=100]\tLoss=0.2282\tacc=0.9223\t3719.0 examples/second\n",
      "[Step=150]\tLoss=0.2289\tacc=0.9225\t3393.7 examples/second\n",
      "Test Loss=0.3506, Test acc=0.8825\n",
      "Saving...\n",
      "\n",
      "Epoch: 19\n",
      "[Step=50]\tLoss=0.2149\tacc=0.9284\t3375.9 examples/second\n",
      "[Step=100]\tLoss=0.2249\tacc=0.9246\t3018.9 examples/second\n",
      "[Step=150]\tLoss=0.2244\tacc=0.9237\t3088.9 examples/second\n",
      "Test Loss=0.3458, Test acc=0.8847\n",
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "best_acc = 0.\n",
    "for epoch in range(20):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    q=(epoch+1)*8\n",
    "    \n",
    "    net.train()\n",
    "    # Increase model sparsity\n",
    "    if epoch<10:\n",
    "        global_prune_by_percentage(net, q=q)\n",
    "    if epoch<9:\n",
    "        finetune_after_prune(net, trainloader, criterion, optimizer,prune=False)\n",
    "    else:\n",
    "        finetune_after_prune(net, trainloader, criterion, optimizer)\n",
    "    \n",
    "    #Start the testing code.\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    num_val_steps = len(testloader)\n",
    "    val_acc = correct / total\n",
    "    print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n",
    "    \n",
    "    if epoch>=10:\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(\"Saving...\")\n",
    "            torch.save(net.state_dict(), \"net_after_global_iterative_prune.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of head_conv.0.conv: 0.3101851851851852\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.6571180555555556\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.6397569444444444\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.6258680555555556\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.6467013888888888\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.6315104166666666\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.6697048611111112\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.6254340277777778\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.6886935763888888\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.7252604166666666\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.7822265625\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.7248263888888888\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.8127170138888888\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.7322591145833334\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.7645399305555556\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.7768825954861112\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.826416015625\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.8526204427083334\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.9767523871527778\n",
      "Sparsity of final_fc.linear: 0.1578125\n",
      "Total sparsity of: 0.7999970186631685\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3458, Test accuracy=0.8847\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"net_after_global_iterative_prune.pt\"))\n",
    "\n",
    "zeros_sum = 0\n",
    "total_sum = 0\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        # Your code here:\n",
    "        # Convert the weight of \"layer\" to numpy array\n",
    "        out = layer.weight\n",
    "        np_weight = out.cpu().detach().numpy()\n",
    "        N = np_weight.size\n",
    "        # Count number of zeros\n",
    "        zeros = N-np.count_nonzero(np_weight)\n",
    "        # Count number of parameters\n",
    "        total = N\n",
    "        zeros_sum+=zeros\n",
    "        total_sum+=total\n",
    "        print('Sparsity of '+name+': '+str(zeros/total))\n",
    "print('Total sparsity of: '+str(zeros_sum/total_sum))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3 (b) and (c): Fixed-point quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3861, Test accuracy=0.8972\n"
     ]
    }
   ],
   "source": [
    "# Define quantized model and load weight\n",
    "Nbits = 4 #Change this value to finish (b) and (c)\n",
    "\n",
    "net = ResNetCIFAR(num_layers=20, Nbits=Nbits)\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "test(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.0631\tacc=0.9791\t1160.8 examples/second\n",
      "[Step=100]\tLoss=0.0627\tacc=0.9793\t1084.8 examples/second\n",
      "[Step=150]\tLoss=0.0635\tacc=0.9790\t1087.4 examples/second\n",
      "Test Loss=0.3351, Test acc=0.9090\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.0610\tacc=0.9795\t486.5 examples/second\n",
      "[Step=250]\tLoss=0.0657\tacc=0.9776\t1208.4 examples/second\n",
      "[Step=300]\tLoss=0.0643\tacc=0.9780\t1091.5 examples/second\n",
      "[Step=350]\tLoss=0.0638\tacc=0.9778\t1107.5 examples/second\n",
      "Test Loss=0.3328, Test acc=0.9115\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.0627\tacc=0.9775\t462.1 examples/second\n",
      "[Step=450]\tLoss=0.0626\tacc=0.9792\t1162.2 examples/second\n",
      "[Step=500]\tLoss=0.0620\tacc=0.9788\t1153.1 examples/second\n",
      "[Step=550]\tLoss=0.0604\tacc=0.9793\t1036.6 examples/second\n",
      "Test Loss=0.3312, Test acc=0.9116\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.0604\tacc=0.9824\t470.0 examples/second\n",
      "[Step=650]\tLoss=0.0618\tacc=0.9800\t913.4 examples/second\n",
      "[Step=700]\tLoss=0.0608\tacc=0.9797\t820.9 examples/second\n",
      "[Step=750]\tLoss=0.0595\tacc=0.9801\t1124.1 examples/second\n",
      "Test Loss=0.3345, Test acc=0.9109\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.0499\tacc=0.9849\t408.3 examples/second\n",
      "[Step=850]\tLoss=0.0584\tacc=0.9799\t1132.0 examples/second\n",
      "[Step=900]\tLoss=0.0603\tacc=0.9795\t1159.5 examples/second\n",
      "[Step=950]\tLoss=0.0592\tacc=0.9800\t1184.3 examples/second\n",
      "Test Loss=0.3289, Test acc=0.9137\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.0563\tacc=0.9818\t455.7 examples/second\n",
      "[Step=1050]\tLoss=0.0540\tacc=0.9830\t1158.1 examples/second\n",
      "[Step=1100]\tLoss=0.0558\tacc=0.9817\t1130.8 examples/second\n",
      "[Step=1150]\tLoss=0.0574\tacc=0.9809\t1139.8 examples/second\n",
      "Test Loss=0.3334, Test acc=0.9124\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.0546\tacc=0.9819\t462.9 examples/second\n",
      "[Step=1250]\tLoss=0.0566\tacc=0.9815\t1174.6 examples/second\n",
      "[Step=1300]\tLoss=0.0560\tacc=0.9814\t1014.2 examples/second\n",
      "[Step=1350]\tLoss=0.0567\tacc=0.9809\t1143.3 examples/second\n",
      "Test Loss=0.3307, Test acc=0.9125\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.0572\tacc=0.9817\t437.2 examples/second\n",
      "[Step=1450]\tLoss=0.0569\tacc=0.9817\t1230.7 examples/second\n",
      "[Step=1500]\tLoss=0.0568\tacc=0.9812\t1098.3 examples/second\n",
      "[Step=1550]\tLoss=0.0552\tacc=0.9819\t1114.0 examples/second\n",
      "Test Loss=0.3324, Test acc=0.9133\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.0557\tacc=0.9818\t484.2 examples/second\n",
      "[Step=1650]\tLoss=0.0533\tacc=0.9825\t1130.3 examples/second\n",
      "[Step=1700]\tLoss=0.0539\tacc=0.9820\t944.8 examples/second\n",
      "[Step=1750]\tLoss=0.0552\tacc=0.9815\t1154.6 examples/second\n",
      "Test Loss=0.3370, Test acc=0.9118\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.0526\tacc=0.9824\t488.4 examples/second\n",
      "[Step=1850]\tLoss=0.0529\tacc=0.9820\t1085.9 examples/second\n",
      "[Step=1900]\tLoss=0.0536\tacc=0.9820\t1040.8 examples/second\n",
      "[Step=1950]\tLoss=0.0542\tacc=0.9816\t1151.9 examples/second\n",
      "Test Loss=0.3322, Test acc=0.9134\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.0563\tacc=0.9822\t498.7 examples/second\n",
      "[Step=2050]\tLoss=0.0546\tacc=0.9821\t1090.3 examples/second\n",
      "[Step=2100]\tLoss=0.0562\tacc=0.9816\t995.1 examples/second\n",
      "[Step=2150]\tLoss=0.0565\tacc=0.9816\t1248.4 examples/second\n",
      "Test Loss=0.3380, Test acc=0.9125\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.0571\tacc=0.9811\t489.9 examples/second\n",
      "[Step=2250]\tLoss=0.0552\tacc=0.9816\t1260.8 examples/second\n",
      "[Step=2300]\tLoss=0.0568\tacc=0.9811\t1223.7 examples/second\n",
      "[Step=2350]\tLoss=0.0563\tacc=0.9814\t1120.5 examples/second\n",
      "Test Loss=0.3337, Test acc=0.9125\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.0545\tacc=0.9820\t470.8 examples/second\n",
      "[Step=2450]\tLoss=0.0535\tacc=0.9821\t1151.8 examples/second\n",
      "[Step=2500]\tLoss=0.0531\tacc=0.9822\t1093.3 examples/second\n",
      "Test Loss=0.3402, Test acc=0.9115\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.0572\tacc=0.9844\t515.1 examples/second\n",
      "[Step=2600]\tLoss=0.0537\tacc=0.9838\t1177.4 examples/second\n",
      "[Step=2650]\tLoss=0.0538\tacc=0.9833\t1098.0 examples/second\n",
      "[Step=2700]\tLoss=0.0552\tacc=0.9826\t1049.0 examples/second\n",
      "Test Loss=0.3301, Test acc=0.9132\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.0586\tacc=0.9772\t510.5 examples/second\n",
      "[Step=2800]\tLoss=0.0553\tacc=0.9815\t1257.3 examples/second\n",
      "[Step=2850]\tLoss=0.0559\tacc=0.9820\t1110.9 examples/second\n",
      "[Step=2900]\tLoss=0.0555\tacc=0.9821\t1027.3 examples/second\n",
      "Test Loss=0.3407, Test acc=0.9107\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.0551\tacc=0.9797\t473.7 examples/second\n",
      "[Step=3000]\tLoss=0.0557\tacc=0.9805\t1093.7 examples/second\n",
      "[Step=3050]\tLoss=0.0544\tacc=0.9814\t1087.5 examples/second\n",
      "[Step=3100]\tLoss=0.0546\tacc=0.9816\t1018.2 examples/second\n",
      "Test Loss=0.3419, Test acc=0.9112\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.0464\tacc=0.9847\t505.4 examples/second\n",
      "[Step=3200]\tLoss=0.0532\tacc=0.9823\t1177.6 examples/second\n",
      "[Step=3250]\tLoss=0.0544\tacc=0.9820\t1077.8 examples/second\n",
      "[Step=3300]\tLoss=0.0543\tacc=0.9821\t1102.7 examples/second\n",
      "Test Loss=0.3360, Test acc=0.9122\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.0488\tacc=0.9822\t478.7 examples/second\n",
      "[Step=3400]\tLoss=0.0514\tacc=0.9818\t1124.6 examples/second\n",
      "[Step=3450]\tLoss=0.0524\tacc=0.9821\t1223.4 examples/second\n",
      "[Step=3500]\tLoss=0.0527\tacc=0.9823\t1089.2 examples/second\n",
      "Test Loss=0.3385, Test acc=0.9109\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.0571\tacc=0.9810\t510.1 examples/second\n",
      "[Step=3600]\tLoss=0.0538\tacc=0.9829\t1212.5 examples/second\n",
      "[Step=3650]\tLoss=0.0527\tacc=0.9833\t1004.7 examples/second\n",
      "[Step=3700]\tLoss=0.0534\tacc=0.9829\t1100.6 examples/second\n",
      "Test Loss=0.3391, Test acc=0.9112\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.0492\tacc=0.9853\t486.7 examples/second\n",
      "[Step=3800]\tLoss=0.0533\tacc=0.9829\t1088.3 examples/second\n",
      "[Step=3850]\tLoss=0.0525\tacc=0.9829\t1040.9 examples/second\n",
      "[Step=3900]\tLoss=0.0521\tacc=0.9829\t1276.1 examples/second\n",
      "Test Loss=0.3337, Test acc=0.9129\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3289, Test accuracy=0.9137\n"
     ]
    }
   ],
   "source": [
    "# Quantized model finetuning\n",
    "finetune(net, epochs=20, batch_size=256, lr=0.002, reg=1e-4)   \n",
    "\n",
    "# Load the model with best accuracy\n",
    "net.load_state_dict(torch.load(\"quantized_net_after_finetune.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Nbits =  2 --------------------------------------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Test Loss=9.5441, Test accuracy=0.0899\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=1.3362\tacc=0.6270\t1361.4 examples/second\n",
      "[Step=100]\tLoss=1.0766\tacc=0.6844\t1145.8 examples/second\n",
      "[Step=150]\tLoss=0.9446\tacc=0.7149\t1324.1 examples/second\n",
      "Test Loss=0.7871, Test acc=0.7617\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.5654\tacc=0.8135\t520.8 examples/second\n",
      "[Step=250]\tLoss=0.5645\tacc=0.8120\t1224.2 examples/second\n",
      "[Step=300]\tLoss=0.5453\tacc=0.8176\t1113.2 examples/second\n",
      "[Step=350]\tLoss=0.5343\tacc=0.8194\t1096.9 examples/second\n",
      "Test Loss=0.6190, Test acc=0.8047\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.4880\tacc=0.8330\t527.8 examples/second\n",
      "[Step=450]\tLoss=0.4867\tacc=0.8337\t1189.1 examples/second\n",
      "[Step=500]\tLoss=0.4824\tacc=0.8361\t1124.4 examples/second\n",
      "[Step=550]\tLoss=0.4695\tacc=0.8401\t1112.0 examples/second\n",
      "Test Loss=0.5782, Test acc=0.8212\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.4458\tacc=0.8464\t503.6 examples/second\n",
      "[Step=650]\tLoss=0.4382\tacc=0.8481\t1171.0 examples/second\n",
      "[Step=700]\tLoss=0.4299\tacc=0.8509\t1133.6 examples/second\n",
      "[Step=750]\tLoss=0.4251\tacc=0.8521\t1033.0 examples/second\n",
      "Test Loss=0.5841, Test acc=0.8179\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.3981\tacc=0.8584\t494.4 examples/second\n",
      "[Step=850]\tLoss=0.4004\tacc=0.8625\t1140.4 examples/second\n",
      "[Step=900]\tLoss=0.3977\tacc=0.8628\t1088.0 examples/second\n",
      "[Step=950]\tLoss=0.3931\tacc=0.8637\t1178.7 examples/second\n",
      "Test Loss=0.5482, Test acc=0.8294\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.3829\tacc=0.8688\t502.3 examples/second\n",
      "[Step=1050]\tLoss=0.3805\tacc=0.8675\t1090.4 examples/second\n",
      "[Step=1100]\tLoss=0.3759\tacc=0.8696\t996.4 examples/second\n",
      "[Step=1150]\tLoss=0.3739\tacc=0.8709\t1098.9 examples/second\n",
      "Test Loss=0.5394, Test acc=0.8338\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.3600\tacc=0.8781\t485.1 examples/second\n",
      "[Step=1250]\tLoss=0.3729\tacc=0.8726\t1178.6 examples/second\n",
      "[Step=1300]\tLoss=0.3679\tacc=0.8719\t1016.6 examples/second\n",
      "[Step=1350]\tLoss=0.3605\tacc=0.8742\t1141.0 examples/second\n",
      "Test Loss=0.6477, Test acc=0.8123\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.3493\tacc=0.8779\t517.0 examples/second\n",
      "[Step=1450]\tLoss=0.3489\tacc=0.8758\t1227.1 examples/second\n",
      "[Step=1500]\tLoss=0.3532\tacc=0.8737\t1028.7 examples/second\n",
      "[Step=1550]\tLoss=0.3506\tacc=0.8755\t1265.4 examples/second\n",
      "Test Loss=0.6244, Test acc=0.8203\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.3453\tacc=0.8767\t504.8 examples/second\n",
      "[Step=1650]\tLoss=0.3341\tacc=0.8820\t1312.7 examples/second\n",
      "[Step=1700]\tLoss=0.3331\tacc=0.8824\t1069.3 examples/second\n",
      "[Step=1750]\tLoss=0.3358\tacc=0.8808\t1262.4 examples/second\n",
      "Test Loss=0.5031, Test acc=0.8467\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.3230\tacc=0.8875\t527.5 examples/second\n",
      "[Step=1850]\tLoss=0.3236\tacc=0.8871\t1221.4 examples/second\n",
      "[Step=1900]\tLoss=0.3274\tacc=0.8851\t1126.9 examples/second\n",
      "[Step=1950]\tLoss=0.3254\tacc=0.8860\t1148.3 examples/second\n",
      "Test Loss=0.5141, Test acc=0.8470\n",
      "Saving...\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.3281\tacc=0.8859\t513.7 examples/second\n",
      "[Step=2050]\tLoss=0.3203\tacc=0.8895\t1236.5 examples/second\n",
      "[Step=2100]\tLoss=0.3189\tacc=0.8886\t1121.8 examples/second\n",
      "[Step=2150]\tLoss=0.3195\tacc=0.8884\t1294.4 examples/second\n",
      "Test Loss=0.4522, Test acc=0.8547\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.3275\tacc=0.8844\t469.7 examples/second\n",
      "[Step=2250]\tLoss=0.3215\tacc=0.8860\t1194.9 examples/second\n",
      "[Step=2300]\tLoss=0.3184\tacc=0.8867\t1128.2 examples/second\n",
      "[Step=2350]\tLoss=0.3148\tacc=0.8886\t1278.5 examples/second\n",
      "Test Loss=0.4648, Test acc=0.8549\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.3073\tacc=0.8890\t507.4 examples/second\n",
      "[Step=2450]\tLoss=0.3051\tacc=0.8893\t1347.7 examples/second\n",
      "[Step=2500]\tLoss=0.3068\tacc=0.8899\t1042.6 examples/second\n",
      "Test Loss=0.5407, Test acc=0.8379\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.2873\tacc=0.8984\t494.9 examples/second\n",
      "[Step=2600]\tLoss=0.2986\tacc=0.8934\t1363.2 examples/second\n",
      "[Step=2650]\tLoss=0.3010\tacc=0.8929\t1180.3 examples/second\n",
      "[Step=2700]\tLoss=0.3061\tacc=0.8913\t1145.7 examples/second\n",
      "Test Loss=0.4558, Test acc=0.8570\n",
      "Saving...\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.2885\tacc=0.8906\t541.9 examples/second\n",
      "[Step=2800]\tLoss=0.2989\tacc=0.8927\t1416.4 examples/second\n",
      "[Step=2850]\tLoss=0.2958\tacc=0.8945\t1300.5 examples/second\n",
      "[Step=2900]\tLoss=0.2994\tacc=0.8937\t1061.9 examples/second\n",
      "Test Loss=0.4956, Test acc=0.8430\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.3075\tacc=0.8922\t517.7 examples/second\n",
      "[Step=3000]\tLoss=0.2943\tacc=0.8956\t1257.3 examples/second\n",
      "[Step=3050]\tLoss=0.2938\tacc=0.8968\t1256.4 examples/second\n",
      "[Step=3100]\tLoss=0.2910\tacc=0.8974\t1156.8 examples/second\n",
      "Test Loss=0.5293, Test acc=0.8414\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.2683\tacc=0.9021\t514.4 examples/second\n",
      "[Step=3200]\tLoss=0.2825\tacc=0.9015\t1312.9 examples/second\n",
      "[Step=3250]\tLoss=0.2864\tacc=0.8993\t1251.2 examples/second\n",
      "[Step=3300]\tLoss=0.2874\tacc=0.8992\t1141.3 examples/second\n",
      "Test Loss=0.5121, Test acc=0.8463\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.2804\tacc=0.9034\t494.4 examples/second\n",
      "[Step=3400]\tLoss=0.2884\tacc=0.8995\t1168.8 examples/second\n",
      "[Step=3450]\tLoss=0.2849\tacc=0.8995\t1107.2 examples/second\n",
      "[Step=3500]\tLoss=0.2840\tacc=0.8994\t1306.7 examples/second\n",
      "Test Loss=0.4468, Test acc=0.8601\n",
      "Saving...\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.2747\tacc=0.9020\t467.9 examples/second\n",
      "[Step=3600]\tLoss=0.2843\tacc=0.9000\t1249.2 examples/second\n",
      "[Step=3650]\tLoss=0.2856\tacc=0.8998\t1177.8 examples/second\n",
      "[Step=3700]\tLoss=0.2860\tacc=0.8993\t1343.2 examples/second\n",
      "Test Loss=0.4417, Test acc=0.8611\n",
      "Saving...\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.2877\tacc=0.8968\t487.0 examples/second\n",
      "[Step=3800]\tLoss=0.2733\tacc=0.9022\t1109.4 examples/second\n",
      "[Step=3850]\tLoss=0.2716\tacc=0.9036\t1086.1 examples/second\n",
      "[Step=3900]\tLoss=0.2758\tacc=0.9024\t1286.2 examples/second\n",
      "Test Loss=0.5400, Test acc=0.8387\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.4417, Test accuracy=0.8611\n",
      "-------------------- Nbits =  3 --------------------------------------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9874, Test accuracy=0.7662\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.1630\tacc=0.9390\t1171.1 examples/second\n",
      "[Step=100]\tLoss=0.1587\tacc=0.9420\t1089.9 examples/second\n",
      "[Step=150]\tLoss=0.1538\tacc=0.9447\t1107.3 examples/second\n",
      "Test Loss=0.3924, Test acc=0.8943\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.1269\tacc=0.9512\t519.4 examples/second\n",
      "[Step=250]\tLoss=0.1259\tacc=0.9533\t1098.3 examples/second\n",
      "[Step=300]\tLoss=0.1237\tacc=0.9540\t1177.6 examples/second\n",
      "[Step=350]\tLoss=0.1215\tacc=0.9556\t1135.7 examples/second\n",
      "Test Loss=0.3863, Test acc=0.8980\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.1045\tacc=0.9590\t510.5 examples/second\n",
      "[Step=450]\tLoss=0.1122\tacc=0.9611\t1235.5 examples/second\n",
      "[Step=500]\tLoss=0.1115\tacc=0.9602\t1221.4 examples/second\n",
      "[Step=550]\tLoss=0.1126\tacc=0.9592\t1005.0 examples/second\n",
      "Test Loss=0.3839, Test acc=0.8982\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.1136\tacc=0.9580\t467.0 examples/second\n",
      "[Step=650]\tLoss=0.1138\tacc=0.9586\t1303.9 examples/second\n",
      "[Step=700]\tLoss=0.1097\tacc=0.9601\t1147.5 examples/second\n",
      "[Step=750]\tLoss=0.1084\tacc=0.9609\t1092.5 examples/second\n",
      "Test Loss=0.3666, Test acc=0.9019\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.1100\tacc=0.9609\t496.9 examples/second\n",
      "[Step=850]\tLoss=0.1114\tacc=0.9610\t1151.2 examples/second\n",
      "[Step=900]\tLoss=0.1079\tacc=0.9618\t1111.2 examples/second\n",
      "[Step=950]\tLoss=0.1052\tacc=0.9629\t1082.1 examples/second\n",
      "Test Loss=0.3740, Test acc=0.9006\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.1067\tacc=0.9619\t478.9 examples/second\n",
      "[Step=1050]\tLoss=0.1045\tacc=0.9624\t1142.1 examples/second\n",
      "[Step=1100]\tLoss=0.1046\tacc=0.9623\t1128.4 examples/second\n",
      "[Step=1150]\tLoss=0.1051\tacc=0.9621\t1145.1 examples/second\n",
      "Test Loss=0.3688, Test acc=0.9023\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.0949\tacc=0.9684\t495.9 examples/second\n",
      "[Step=1250]\tLoss=0.1008\tacc=0.9645\t1275.6 examples/second\n",
      "[Step=1300]\tLoss=0.1044\tacc=0.9623\t1097.6 examples/second\n",
      "[Step=1350]\tLoss=0.1008\tacc=0.9636\t1250.0 examples/second\n",
      "Test Loss=0.3772, Test acc=0.9001\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.0963\tacc=0.9632\t497.9 examples/second\n",
      "[Step=1450]\tLoss=0.0987\tacc=0.9634\t1099.0 examples/second\n",
      "[Step=1500]\tLoss=0.0991\tacc=0.9640\t1056.9 examples/second\n",
      "[Step=1550]\tLoss=0.0976\tacc=0.9644\t1222.8 examples/second\n",
      "Test Loss=0.3712, Test acc=0.9019\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.0916\tacc=0.9685\t497.8 examples/second\n",
      "[Step=1650]\tLoss=0.0945\tacc=0.9673\t1109.7 examples/second\n",
      "[Step=1700]\tLoss=0.0939\tacc=0.9667\t1068.2 examples/second\n",
      "[Step=1750]\tLoss=0.0937\tacc=0.9666\t1114.5 examples/second\n",
      "Test Loss=0.3709, Test acc=0.9035\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.0887\tacc=0.9682\t476.8 examples/second\n",
      "[Step=1850]\tLoss=0.0902\tacc=0.9684\t1192.8 examples/second\n",
      "[Step=1900]\tLoss=0.0910\tacc=0.9683\t1077.3 examples/second\n",
      "[Step=1950]\tLoss=0.0929\tacc=0.9677\t1134.8 examples/second\n",
      "Test Loss=0.3612, Test acc=0.9045\n",
      "Saving...\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.0928\tacc=0.9660\t487.2 examples/second\n",
      "[Step=2050]\tLoss=0.0914\tacc=0.9669\t1233.5 examples/second\n",
      "[Step=2100]\tLoss=0.0929\tacc=0.9663\t1148.5 examples/second\n",
      "[Step=2150]\tLoss=0.0924\tacc=0.9668\t1160.1 examples/second\n",
      "Test Loss=0.3645, Test acc=0.9035\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.0856\tacc=0.9683\t466.7 examples/second\n",
      "[Step=2250]\tLoss=0.0883\tacc=0.9687\t1119.0 examples/second\n",
      "[Step=2300]\tLoss=0.0898\tacc=0.9680\t1028.9 examples/second\n",
      "[Step=2350]\tLoss=0.0900\tacc=0.9683\t1176.5 examples/second\n",
      "Test Loss=0.3586, Test acc=0.9048\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.0890\tacc=0.9679\t511.9 examples/second\n",
      "[Step=2450]\tLoss=0.0913\tacc=0.9674\t1040.4 examples/second\n",
      "[Step=2500]\tLoss=0.0938\tacc=0.9671\t1054.8 examples/second\n",
      "Test Loss=0.3607, Test acc=0.9032\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.0966\tacc=0.9707\t493.5 examples/second\n",
      "[Step=2600]\tLoss=0.0864\tacc=0.9702\t1087.5 examples/second\n",
      "[Step=2650]\tLoss=0.0878\tacc=0.9696\t1065.3 examples/second\n",
      "[Step=2700]\tLoss=0.0907\tacc=0.9679\t1032.5 examples/second\n",
      "Test Loss=0.3689, Test acc=0.9036\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.0809\tacc=0.9707\t511.7 examples/second\n",
      "[Step=2800]\tLoss=0.0914\tacc=0.9664\t1275.8 examples/second\n",
      "[Step=2850]\tLoss=0.0915\tacc=0.9677\t1152.4 examples/second\n",
      "[Step=2900]\tLoss=0.0892\tacc=0.9686\t1129.8 examples/second\n",
      "Test Loss=0.3695, Test acc=0.9016\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.0885\tacc=0.9676\t507.7 examples/second\n",
      "[Step=3000]\tLoss=0.0882\tacc=0.9686\t1177.7 examples/second\n",
      "[Step=3050]\tLoss=0.0899\tacc=0.9675\t1132.8 examples/second\n",
      "[Step=3100]\tLoss=0.0884\tacc=0.9681\t1234.0 examples/second\n",
      "Test Loss=0.3585, Test acc=0.9056\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.0902\tacc=0.9701\t528.6 examples/second\n",
      "[Step=3200]\tLoss=0.0848\tacc=0.9706\t1348.2 examples/second\n",
      "[Step=3250]\tLoss=0.0868\tacc=0.9697\t1173.9 examples/second\n",
      "[Step=3300]\tLoss=0.0865\tacc=0.9699\t1097.4 examples/second\n",
      "Test Loss=0.3568, Test acc=0.9041\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.0783\tacc=0.9701\t505.2 examples/second\n",
      "[Step=3400]\tLoss=0.0868\tacc=0.9678\t1102.3 examples/second\n",
      "[Step=3450]\tLoss=0.0868\tacc=0.9689\t1110.6 examples/second\n",
      "[Step=3500]\tLoss=0.0881\tacc=0.9685\t1135.9 examples/second\n",
      "Test Loss=0.3698, Test acc=0.9009\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.0865\tacc=0.9684\t488.2 examples/second\n",
      "[Step=3600]\tLoss=0.0832\tacc=0.9701\t1123.6 examples/second\n",
      "[Step=3650]\tLoss=0.0821\tacc=0.9710\t1068.5 examples/second\n",
      "[Step=3700]\tLoss=0.0826\tacc=0.9707\t1135.8 examples/second\n",
      "Test Loss=0.3626, Test acc=0.9036\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.0918\tacc=0.9681\t487.0 examples/second\n",
      "[Step=3800]\tLoss=0.0863\tacc=0.9697\t1296.2 examples/second\n",
      "[Step=3850]\tLoss=0.0838\tacc=0.9708\t1119.6 examples/second\n",
      "[Step=3900]\tLoss=0.0858\tacc=0.9699\t1080.9 examples/second\n",
      "Test Loss=0.3568, Test acc=0.9035\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3585, Test accuracy=0.9056\n",
      "-------------------- Nbits =  4 --------------------------------------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3861, Test accuracy=0.8972\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.0666\tacc=0.9772\t1239.7 examples/second\n",
      "[Step=100]\tLoss=0.0671\tacc=0.9773\t1183.0 examples/second\n",
      "[Step=150]\tLoss=0.0656\tacc=0.9775\t1138.2 examples/second\n",
      "Test Loss=0.3348, Test acc=0.9092\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.0968\tacc=0.9658\t491.6 examples/second\n",
      "[Step=250]\tLoss=0.0636\tacc=0.9790\t1127.3 examples/second\n",
      "[Step=300]\tLoss=0.0619\tacc=0.9794\t1085.5 examples/second\n",
      "[Step=350]\tLoss=0.0628\tacc=0.9787\t1143.3 examples/second\n",
      "Test Loss=0.3337, Test acc=0.9109\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.0654\tacc=0.9741\t523.8 examples/second\n",
      "[Step=450]\tLoss=0.0623\tacc=0.9789\t1115.0 examples/second\n",
      "[Step=500]\tLoss=0.0615\tacc=0.9792\t1153.8 examples/second\n",
      "[Step=550]\tLoss=0.0612\tacc=0.9795\t1148.0 examples/second\n",
      "Test Loss=0.3333, Test acc=0.9105\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.0573\tacc=0.9811\t531.8 examples/second\n",
      "[Step=650]\tLoss=0.0553\tacc=0.9815\t1249.1 examples/second\n",
      "[Step=700]\tLoss=0.0570\tacc=0.9809\t1201.5 examples/second\n",
      "[Step=750]\tLoss=0.0589\tacc=0.9804\t1135.7 examples/second\n",
      "Test Loss=0.3319, Test acc=0.9106\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.0531\tacc=0.9819\t499.9 examples/second\n",
      "[Step=850]\tLoss=0.0601\tacc=0.9798\t1092.6 examples/second\n",
      "[Step=900]\tLoss=0.0594\tacc=0.9801\t1120.9 examples/second\n",
      "[Step=950]\tLoss=0.0603\tacc=0.9800\t1081.5 examples/second\n",
      "Test Loss=0.3318, Test acc=0.9108\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.0575\tacc=0.9816\t508.1 examples/second\n",
      "[Step=1050]\tLoss=0.0597\tacc=0.9795\t1164.8 examples/second\n",
      "[Step=1100]\tLoss=0.0592\tacc=0.9800\t1083.0 examples/second\n",
      "[Step=1150]\tLoss=0.0586\tacc=0.9806\t1417.2 examples/second\n",
      "Test Loss=0.3340, Test acc=0.9102\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.0582\tacc=0.9801\t515.9 examples/second\n",
      "[Step=1250]\tLoss=0.0553\tacc=0.9822\t1114.9 examples/second\n",
      "[Step=1300]\tLoss=0.0568\tacc=0.9810\t1088.7 examples/second\n",
      "[Step=1350]\tLoss=0.0574\tacc=0.9810\t1094.5 examples/second\n",
      "Test Loss=0.3305, Test acc=0.9120\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.0570\tacc=0.9803\t499.7 examples/second\n",
      "[Step=1450]\tLoss=0.0560\tacc=0.9807\t1201.8 examples/second\n",
      "[Step=1500]\tLoss=0.0574\tacc=0.9807\t1062.3 examples/second\n",
      "[Step=1550]\tLoss=0.0561\tacc=0.9810\t1118.8 examples/second\n",
      "Test Loss=0.3302, Test acc=0.9119\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.0627\tacc=0.9795\t509.2 examples/second\n",
      "[Step=1650]\tLoss=0.0583\tacc=0.9808\t1405.9 examples/second\n",
      "[Step=1700]\tLoss=0.0572\tacc=0.9814\t1006.2 examples/second\n",
      "[Step=1750]\tLoss=0.0578\tacc=0.9808\t1104.4 examples/second\n",
      "Test Loss=0.3365, Test acc=0.9116\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.0588\tacc=0.9812\t472.7 examples/second\n",
      "[Step=1850]\tLoss=0.0585\tacc=0.9810\t1111.5 examples/second\n",
      "[Step=1900]\tLoss=0.0579\tacc=0.9815\t1065.9 examples/second\n",
      "[Step=1950]\tLoss=0.0575\tacc=0.9813\t1251.1 examples/second\n",
      "Test Loss=0.3333, Test acc=0.9110\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.0524\tacc=0.9825\t488.1 examples/second\n",
      "[Step=2050]\tLoss=0.0558\tacc=0.9816\t1141.8 examples/second\n",
      "[Step=2100]\tLoss=0.0550\tacc=0.9816\t1107.9 examples/second\n",
      "[Step=2150]\tLoss=0.0563\tacc=0.9812\t1129.3 examples/second\n",
      "Test Loss=0.3341, Test acc=0.9137\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.0549\tacc=0.9823\t471.0 examples/second\n",
      "[Step=2250]\tLoss=0.0554\tacc=0.9813\t1195.9 examples/second\n",
      "[Step=2300]\tLoss=0.0543\tacc=0.9820\t981.0 examples/second\n",
      "[Step=2350]\tLoss=0.0549\tacc=0.9821\t1239.8 examples/second\n",
      "Test Loss=0.3337, Test acc=0.9121\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.0547\tacc=0.9818\t513.2 examples/second\n",
      "[Step=2450]\tLoss=0.0548\tacc=0.9820\t1186.6 examples/second\n",
      "[Step=2500]\tLoss=0.0549\tacc=0.9819\t1086.2 examples/second\n",
      "Test Loss=0.3334, Test acc=0.9127\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.0436\tacc=0.9883\t510.8 examples/second\n",
      "[Step=2600]\tLoss=0.0529\tacc=0.9833\t1187.1 examples/second\n",
      "[Step=2650]\tLoss=0.0558\tacc=0.9820\t1172.8 examples/second\n",
      "[Step=2700]\tLoss=0.0562\tacc=0.9815\t1234.6 examples/second\n",
      "Test Loss=0.3312, Test acc=0.9130\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.0617\tacc=0.9779\t533.9 examples/second\n",
      "[Step=2800]\tLoss=0.0554\tacc=0.9821\t1298.9 examples/second\n",
      "[Step=2850]\tLoss=0.0565\tacc=0.9810\t1250.3 examples/second\n",
      "[Step=2900]\tLoss=0.0551\tacc=0.9816\t1048.8 examples/second\n",
      "Test Loss=0.3332, Test acc=0.9141\n",
      "Saving...\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.0564\tacc=0.9789\t468.0 examples/second\n",
      "[Step=3000]\tLoss=0.0522\tacc=0.9822\t1019.5 examples/second\n",
      "[Step=3050]\tLoss=0.0530\tacc=0.9820\t1151.0 examples/second\n",
      "[Step=3100]\tLoss=0.0539\tacc=0.9816\t1107.2 examples/second\n",
      "Test Loss=0.3309, Test acc=0.9147\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.0546\tacc=0.9794\t514.7 examples/second\n",
      "[Step=3200]\tLoss=0.0518\tacc=0.9828\t1381.5 examples/second\n",
      "[Step=3250]\tLoss=0.0516\tacc=0.9832\t1159.0 examples/second\n",
      "[Step=3300]\tLoss=0.0524\tacc=0.9829\t1087.3 examples/second\n",
      "Test Loss=0.3342, Test acc=0.9120\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.0549\tacc=0.9820\t497.8 examples/second\n",
      "[Step=3400]\tLoss=0.0556\tacc=0.9813\t1197.2 examples/second\n",
      "[Step=3450]\tLoss=0.0539\tacc=0.9821\t1094.9 examples/second\n",
      "[Step=3500]\tLoss=0.0545\tacc=0.9821\t1130.4 examples/second\n",
      "Test Loss=0.3359, Test acc=0.9134\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.0524\tacc=0.9826\t513.4 examples/second\n",
      "[Step=3600]\tLoss=0.0530\tacc=0.9820\t1211.9 examples/second\n",
      "[Step=3650]\tLoss=0.0533\tacc=0.9820\t1122.4 examples/second\n",
      "[Step=3700]\tLoss=0.0531\tacc=0.9823\t1146.6 examples/second\n",
      "Test Loss=0.3379, Test acc=0.9141\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.0533\tacc=0.9830\t519.6 examples/second\n",
      "[Step=3800]\tLoss=0.0522\tacc=0.9838\t1320.3 examples/second\n",
      "[Step=3850]\tLoss=0.0517\tacc=0.9840\t1089.5 examples/second\n",
      "[Step=3900]\tLoss=0.0527\tacc=0.9833\t1074.4 examples/second\n",
      "Test Loss=0.3365, Test acc=0.9133\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3309, Test accuracy=0.9147\n",
      "-------------------- Nbits =  5 --------------------------------------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3390, Test accuracy=0.9112\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.0494\tacc=0.9847\t1142.0 examples/second\n",
      "[Step=100]\tLoss=0.0523\tacc=0.9836\t1203.8 examples/second\n",
      "[Step=150]\tLoss=0.0524\tacc=0.9829\t1127.9 examples/second\n",
      "Test Loss=0.3290, Test acc=0.9131\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.0427\tacc=0.9893\t512.2 examples/second\n",
      "[Step=250]\tLoss=0.0498\tacc=0.9832\t1135.9 examples/second\n",
      "[Step=300]\tLoss=0.0512\tacc=0.9831\t1254.5 examples/second\n",
      "[Step=350]\tLoss=0.0512\tacc=0.9833\t1109.6 examples/second\n",
      "Test Loss=0.3287, Test acc=0.9130\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.0439\tacc=0.9854\t515.7 examples/second\n",
      "[Step=450]\tLoss=0.0499\tacc=0.9839\t1300.2 examples/second\n",
      "[Step=500]\tLoss=0.0518\tacc=0.9831\t1132.4 examples/second\n",
      "[Step=550]\tLoss=0.0507\tacc=0.9833\t1210.8 examples/second\n",
      "Test Loss=0.3281, Test acc=0.9139\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.0450\tacc=0.9873\t494.2 examples/second\n",
      "[Step=650]\tLoss=0.0503\tacc=0.9843\t1325.3 examples/second\n",
      "[Step=700]\tLoss=0.0498\tacc=0.9844\t1281.0 examples/second\n",
      "[Step=750]\tLoss=0.0499\tacc=0.9846\t1126.5 examples/second\n",
      "Test Loss=0.3308, Test acc=0.9138\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.0479\tacc=0.9849\t532.1 examples/second\n",
      "[Step=850]\tLoss=0.0484\tacc=0.9850\t1264.2 examples/second\n",
      "[Step=900]\tLoss=0.0475\tacc=0.9852\t1067.5 examples/second\n",
      "[Step=950]\tLoss=0.0497\tacc=0.9839\t1154.8 examples/second\n",
      "Test Loss=0.3293, Test acc=0.9134\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.0524\tacc=0.9842\t539.2 examples/second\n",
      "[Step=1050]\tLoss=0.0492\tacc=0.9840\t1166.6 examples/second\n",
      "[Step=1100]\tLoss=0.0485\tacc=0.9847\t1168.2 examples/second\n",
      "[Step=1150]\tLoss=0.0472\tacc=0.9850\t1052.0 examples/second\n",
      "Test Loss=0.3305, Test acc=0.9138\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.0505\tacc=0.9837\t538.4 examples/second\n",
      "[Step=1250]\tLoss=0.0492\tacc=0.9849\t1364.7 examples/second\n",
      "[Step=1300]\tLoss=0.0498\tacc=0.9842\t1163.9 examples/second\n",
      "[Step=1350]\tLoss=0.0489\tacc=0.9844\t1226.6 examples/second\n",
      "Test Loss=0.3294, Test acc=0.9150\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.0489\tacc=0.9838\t499.2 examples/second\n",
      "[Step=1450]\tLoss=0.0505\tacc=0.9835\t1206.9 examples/second\n",
      "[Step=1500]\tLoss=0.0492\tacc=0.9842\t1133.8 examples/second\n",
      "[Step=1550]\tLoss=0.0497\tacc=0.9840\t1164.8 examples/second\n",
      "Test Loss=0.3310, Test acc=0.9134\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.0485\tacc=0.9857\t535.7 examples/second\n",
      "[Step=1650]\tLoss=0.0499\tacc=0.9837\t1270.8 examples/second\n",
      "[Step=1700]\tLoss=0.0500\tacc=0.9836\t1062.4 examples/second\n",
      "[Step=1750]\tLoss=0.0500\tacc=0.9833\t1182.2 examples/second\n",
      "Test Loss=0.3327, Test acc=0.9131\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.0483\tacc=0.9832\t493.0 examples/second\n",
      "[Step=1850]\tLoss=0.0467\tacc=0.9845\t1166.8 examples/second\n",
      "[Step=1900]\tLoss=0.0471\tacc=0.9847\t1128.1 examples/second\n",
      "[Step=1950]\tLoss=0.0472\tacc=0.9848\t1347.1 examples/second\n",
      "Test Loss=0.3297, Test acc=0.9135\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.0465\tacc=0.9857\t515.6 examples/second\n",
      "[Step=2050]\tLoss=0.0460\tacc=0.9860\t1090.6 examples/second\n",
      "[Step=2100]\tLoss=0.0466\tacc=0.9852\t1144.4 examples/second\n",
      "[Step=2150]\tLoss=0.0463\tacc=0.9855\t1228.3 examples/second\n",
      "Test Loss=0.3333, Test acc=0.9133\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.0478\tacc=0.9829\t500.9 examples/second\n",
      "[Step=2250]\tLoss=0.0488\tacc=0.9843\t1160.7 examples/second\n",
      "[Step=2300]\tLoss=0.0483\tacc=0.9846\t1145.9 examples/second\n",
      "[Step=2350]\tLoss=0.0480\tacc=0.9848\t1188.1 examples/second\n",
      "Test Loss=0.3316, Test acc=0.9132\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.0494\tacc=0.9842\t509.5 examples/second\n",
      "[Step=2450]\tLoss=0.0480\tacc=0.9848\t1104.6 examples/second\n",
      "[Step=2500]\tLoss=0.0474\tacc=0.9851\t1202.4 examples/second\n",
      "Test Loss=0.3329, Test acc=0.9152\n",
      "Saving...\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.0582\tacc=0.9883\t496.1 examples/second\n",
      "[Step=2600]\tLoss=0.0443\tacc=0.9863\t1269.2 examples/second\n",
      "[Step=2650]\tLoss=0.0465\tacc=0.9855\t1092.5 examples/second\n",
      "[Step=2700]\tLoss=0.0477\tacc=0.9848\t1134.7 examples/second\n",
      "Test Loss=0.3329, Test acc=0.9149\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.0477\tacc=0.9844\t498.3 examples/second\n",
      "[Step=2800]\tLoss=0.0491\tacc=0.9851\t1260.9 examples/second\n",
      "[Step=2850]\tLoss=0.0487\tacc=0.9844\t1212.2 examples/second\n",
      "[Step=2900]\tLoss=0.0479\tacc=0.9847\t1191.8 examples/second\n",
      "Test Loss=0.3345, Test acc=0.9135\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.0418\tacc=0.9863\t523.7 examples/second\n",
      "[Step=3000]\tLoss=0.0467\tacc=0.9856\t1296.7 examples/second\n",
      "[Step=3050]\tLoss=0.0480\tacc=0.9852\t1194.2 examples/second\n",
      "[Step=3100]\tLoss=0.0467\tacc=0.9856\t1089.9 examples/second\n",
      "Test Loss=0.3300, Test acc=0.9141\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.0508\tacc=0.9838\t500.8 examples/second\n",
      "[Step=3200]\tLoss=0.0470\tacc=0.9851\t1308.4 examples/second\n",
      "[Step=3250]\tLoss=0.0472\tacc=0.9851\t1222.8 examples/second\n",
      "[Step=3300]\tLoss=0.0476\tacc=0.9848\t1070.5 examples/second\n",
      "Test Loss=0.3319, Test acc=0.9133\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.0423\tacc=0.9872\t516.8 examples/second\n",
      "[Step=3400]\tLoss=0.0459\tacc=0.9855\t1228.9 examples/second\n",
      "[Step=3450]\tLoss=0.0466\tacc=0.9850\t1124.1 examples/second\n",
      "[Step=3500]\tLoss=0.0459\tacc=0.9854\t1153.5 examples/second\n",
      "Test Loss=0.3340, Test acc=0.9133\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.0475\tacc=0.9837\t492.9 examples/second\n",
      "[Step=3600]\tLoss=0.0477\tacc=0.9837\t1305.7 examples/second\n",
      "[Step=3650]\tLoss=0.0476\tacc=0.9844\t1259.8 examples/second\n",
      "[Step=3700]\tLoss=0.0473\tacc=0.9848\t1244.0 examples/second\n",
      "Test Loss=0.3359, Test acc=0.9129\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.0474\tacc=0.9836\t506.0 examples/second\n",
      "[Step=3800]\tLoss=0.0453\tacc=0.9845\t1196.0 examples/second\n",
      "[Step=3850]\tLoss=0.0453\tacc=0.9854\t1249.7 examples/second\n",
      "[Step=3900]\tLoss=0.0459\tacc=0.9853\t1122.0 examples/second\n",
      "Test Loss=0.3360, Test acc=0.9124\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3329, Test accuracy=0.9152\n",
      "-------------------- Nbits =  6 --------------------------------------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3364, Test accuracy=0.9145\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.0482\tacc=0.9862\t1386.4 examples/second\n",
      "[Step=100]\tLoss=0.0494\tacc=0.9850\t1271.8 examples/second\n",
      "[Step=150]\tLoss=0.0490\tacc=0.9852\t1195.4 examples/second\n",
      "Test Loss=0.3231, Test acc=0.9156\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.0518\tacc=0.9844\t510.1 examples/second\n",
      "[Step=250]\tLoss=0.0480\tacc=0.9838\t1318.4 examples/second\n",
      "[Step=300]\tLoss=0.0483\tacc=0.9837\t1255.0 examples/second\n",
      "[Step=350]\tLoss=0.0475\tacc=0.9845\t1208.9 examples/second\n",
      "Test Loss=0.3271, Test acc=0.9158\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.0525\tacc=0.9814\t532.2 examples/second\n",
      "[Step=450]\tLoss=0.0471\tacc=0.9846\t1303.3 examples/second\n",
      "[Step=500]\tLoss=0.0486\tacc=0.9848\t1266.7 examples/second\n",
      "[Step=550]\tLoss=0.0489\tacc=0.9845\t1270.7 examples/second\n",
      "Test Loss=0.3266, Test acc=0.9159\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.0380\tacc=0.9886\t519.1 examples/second\n",
      "[Step=650]\tLoss=0.0451\tacc=0.9853\t1337.2 examples/second\n",
      "[Step=700]\tLoss=0.0469\tacc=0.9844\t1207.2 examples/second\n",
      "[Step=750]\tLoss=0.0478\tacc=0.9844\t1158.2 examples/second\n",
      "Test Loss=0.3271, Test acc=0.9150\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.0530\tacc=0.9819\t516.8 examples/second\n",
      "[Step=850]\tLoss=0.0489\tacc=0.9837\t1232.9 examples/second\n",
      "[Step=900]\tLoss=0.0484\tacc=0.9842\t1149.0 examples/second\n",
      "[Step=950]\tLoss=0.0480\tacc=0.9845\t1066.3 examples/second\n",
      "Test Loss=0.3271, Test acc=0.9150\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.0477\tacc=0.9859\t503.7 examples/second\n",
      "[Step=1050]\tLoss=0.0474\tacc=0.9853\t1282.6 examples/second\n",
      "[Step=1100]\tLoss=0.0478\tacc=0.9850\t1078.9 examples/second\n",
      "[Step=1150]\tLoss=0.0478\tacc=0.9848\t1203.1 examples/second\n",
      "Test Loss=0.3266, Test acc=0.9140\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.0441\tacc=0.9855\t508.9 examples/second\n",
      "[Step=1250]\tLoss=0.0488\tacc=0.9839\t1214.0 examples/second\n",
      "[Step=1300]\tLoss=0.0484\tacc=0.9840\t1122.9 examples/second\n",
      "[Step=1350]\tLoss=0.0472\tacc=0.9848\t1200.6 examples/second\n",
      "Test Loss=0.3294, Test acc=0.9148\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.0475\tacc=0.9847\t541.4 examples/second\n",
      "[Step=1450]\tLoss=0.0468\tacc=0.9853\t1224.4 examples/second\n",
      "[Step=1500]\tLoss=0.0476\tacc=0.9847\t1070.4 examples/second\n",
      "[Step=1550]\tLoss=0.0471\tacc=0.9851\t1149.0 examples/second\n",
      "Test Loss=0.3292, Test acc=0.9158\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.0434\tacc=0.9858\t547.6 examples/second\n",
      "[Step=1650]\tLoss=0.0462\tacc=0.9852\t1263.1 examples/second\n",
      "[Step=1700]\tLoss=0.0472\tacc=0.9849\t1083.1 examples/second\n",
      "[Step=1750]\tLoss=0.0474\tacc=0.9852\t1317.0 examples/second\n",
      "Test Loss=0.3283, Test acc=0.9150\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.0449\tacc=0.9860\t524.4 examples/second\n",
      "[Step=1850]\tLoss=0.0450\tacc=0.9862\t1338.7 examples/second\n",
      "[Step=1900]\tLoss=0.0452\tacc=0.9858\t1131.1 examples/second\n",
      "[Step=1950]\tLoss=0.0458\tacc=0.9856\t1274.6 examples/second\n",
      "Test Loss=0.3306, Test acc=0.9160\n",
      "Saving...\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.0478\tacc=0.9849\t530.4 examples/second\n",
      "[Step=2050]\tLoss=0.0457\tacc=0.9853\t1178.5 examples/second\n",
      "[Step=2100]\tLoss=0.0462\tacc=0.9855\t1025.2 examples/second\n",
      "[Step=2150]\tLoss=0.0460\tacc=0.9857\t1258.3 examples/second\n",
      "Test Loss=0.3309, Test acc=0.9153\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.0455\tacc=0.9848\t535.3 examples/second\n",
      "[Step=2250]\tLoss=0.0460\tacc=0.9850\t1313.1 examples/second\n",
      "[Step=2300]\tLoss=0.0464\tacc=0.9847\t1300.8 examples/second\n",
      "[Step=2350]\tLoss=0.0470\tacc=0.9848\t1290.8 examples/second\n",
      "Test Loss=0.3300, Test acc=0.9156\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.0482\tacc=0.9850\t555.8 examples/second\n",
      "[Step=2450]\tLoss=0.0473\tacc=0.9853\t1298.3 examples/second\n",
      "[Step=2500]\tLoss=0.0470\tacc=0.9850\t1091.0 examples/second\n",
      "Test Loss=0.3322, Test acc=0.9151\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.0343\tacc=0.9883\t502.3 examples/second\n",
      "[Step=2600]\tLoss=0.0463\tacc=0.9856\t1248.6 examples/second\n",
      "[Step=2650]\tLoss=0.0473\tacc=0.9844\t1296.1 examples/second\n",
      "[Step=2700]\tLoss=0.0474\tacc=0.9843\t1316.9 examples/second\n",
      "Test Loss=0.3313, Test acc=0.9157\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.0383\tacc=0.9883\t563.8 examples/second\n",
      "[Step=2800]\tLoss=0.0463\tacc=0.9850\t1346.0 examples/second\n",
      "[Step=2850]\tLoss=0.0433\tacc=0.9863\t1240.8 examples/second\n",
      "[Step=2900]\tLoss=0.0437\tacc=0.9861\t1116.9 examples/second\n",
      "Test Loss=0.3327, Test acc=0.9139\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.0541\tacc=0.9832\t529.5 examples/second\n",
      "[Step=3000]\tLoss=0.0454\tacc=0.9854\t1423.5 examples/second\n",
      "[Step=3050]\tLoss=0.0458\tacc=0.9853\t1183.7 examples/second\n",
      "[Step=3100]\tLoss=0.0461\tacc=0.9855\t1230.9 examples/second\n",
      "Test Loss=0.3324, Test acc=0.9149\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.0465\tacc=0.9869\t517.1 examples/second\n",
      "[Step=3200]\tLoss=0.0460\tacc=0.9857\t1308.8 examples/second\n",
      "[Step=3250]\tLoss=0.0462\tacc=0.9851\t1173.0 examples/second\n",
      "[Step=3300]\tLoss=0.0460\tacc=0.9853\t1294.7 examples/second\n",
      "Test Loss=0.3355, Test acc=0.9130\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.0505\tacc=0.9820\t548.8 examples/second\n",
      "[Step=3400]\tLoss=0.0431\tacc=0.9855\t1242.4 examples/second\n",
      "[Step=3450]\tLoss=0.0446\tacc=0.9853\t1181.4 examples/second\n",
      "[Step=3500]\tLoss=0.0450\tacc=0.9853\t1062.1 examples/second\n",
      "Test Loss=0.3331, Test acc=0.9132\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.0403\tacc=0.9869\t522.2 examples/second\n",
      "[Step=3600]\tLoss=0.0422\tacc=0.9864\t1233.9 examples/second\n",
      "[Step=3650]\tLoss=0.0438\tacc=0.9856\t1127.3 examples/second\n",
      "[Step=3700]\tLoss=0.0446\tacc=0.9853\t1194.5 examples/second\n",
      "Test Loss=0.3345, Test acc=0.9150\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.0421\tacc=0.9872\t498.8 examples/second\n",
      "[Step=3800]\tLoss=0.0433\tacc=0.9867\t1401.3 examples/second\n",
      "[Step=3850]\tLoss=0.0444\tacc=0.9859\t1161.6 examples/second\n",
      "[Step=3900]\tLoss=0.0448\tacc=0.9858\t1215.7 examples/second\n",
      "Test Loss=0.3343, Test acc=0.9150\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3306, Test accuracy=0.9160\n"
     ]
    }
   ],
   "source": [
    "for Nbits in range(2,7):\n",
    "    print(\"-\"*20,\"Nbits = \",Nbits,\"-\"*80)\n",
    "    net = ResNetCIFAR(num_layers=20, Nbits=Nbits)\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "    test(net)\n",
    "    # Quantized model finetuning\n",
    "    finetune(net, epochs=20, batch_size=256, lr=0.002, reg=1e-4)   \n",
    "\n",
    "    # Load the model with best accuracy\n",
    "    net.load_state_dict(torch.load(\"quantized_net_after_finetune.pt\"))\n",
    "    test(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Nbits =  2 --------------------------------------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Test Loss=7.5443, Test accuracy=0.1085\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=2.3061\tacc=0.1420\t1386.4 examples/second\n",
      "[Step=100]\tLoss=2.2496\tacc=0.1536\t1291.0 examples/second\n",
      "[Step=150]\tLoss=2.2221\tacc=0.1609\t1179.1 examples/second\n",
      "Test Loss=2.1494, Test acc=0.1864\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=2.1508\tacc=0.1904\t581.9 examples/second\n",
      "[Step=250]\tLoss=2.1455\tacc=0.1874\t1267.6 examples/second\n",
      "[Step=300]\tLoss=2.1379\tacc=0.1929\t1365.6 examples/second\n",
      "[Step=350]\tLoss=2.1300\tacc=0.2001\t1109.8 examples/second\n",
      "Test Loss=2.1152, Test acc=0.2111\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=2.1082\tacc=0.2305\t578.6 examples/second\n",
      "[Step=450]\tLoss=2.1042\tacc=0.2208\t1192.5 examples/second\n",
      "[Step=500]\tLoss=2.1029\tacc=0.2223\t1086.6 examples/second\n",
      "[Step=550]\tLoss=2.0981\tacc=0.2235\t1048.0 examples/second\n",
      "Test Loss=2.0990, Test acc=0.2261\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=2.0995\tacc=0.2259\t597.4 examples/second\n",
      "[Step=650]\tLoss=2.0793\tacc=0.2374\t1209.2 examples/second\n",
      "[Step=700]\tLoss=2.0706\tacc=0.2402\t1160.1 examples/second\n",
      "[Step=750]\tLoss=2.0686\tacc=0.2397\t1031.2 examples/second\n",
      "Test Loss=2.0676, Test acc=0.2385\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=2.0559\tacc=0.2554\t572.6 examples/second\n",
      "[Step=850]\tLoss=2.0441\tacc=0.2551\t1189.0 examples/second\n",
      "[Step=900]\tLoss=2.0469\tacc=0.2522\t1264.3 examples/second\n",
      "[Step=950]\tLoss=2.0438\tacc=0.2530\t1318.7 examples/second\n",
      "Test Loss=2.0731, Test acc=0.2258\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=2.0324\tacc=0.2547\t619.0 examples/second\n",
      "[Step=1050]\tLoss=2.0289\tacc=0.2594\t1334.1 examples/second\n",
      "[Step=1100]\tLoss=2.0264\tacc=0.2597\t1269.9 examples/second\n",
      "[Step=1150]\tLoss=2.0234\tacc=0.2611\t1312.7 examples/second\n",
      "Test Loss=2.0371, Test acc=0.2480\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=2.0079\tacc=0.2645\t637.3 examples/second\n",
      "[Step=1250]\tLoss=2.0056\tacc=0.2663\t1380.7 examples/second\n",
      "[Step=1300]\tLoss=2.0056\tacc=0.2654\t1239.8 examples/second\n",
      "[Step=1350]\tLoss=2.0054\tacc=0.2659\t1404.8 examples/second\n",
      "Test Loss=2.0085, Test acc=0.2651\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=1.9938\tacc=0.2690\t590.0 examples/second\n",
      "[Step=1450]\tLoss=1.9895\tacc=0.2710\t1306.6 examples/second\n",
      "[Step=1500]\tLoss=1.9850\tacc=0.2718\t1268.3 examples/second\n",
      "[Step=1550]\tLoss=1.9837\tacc=0.2737\t1464.2 examples/second\n",
      "Test Loss=2.0068, Test acc=0.2598\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=1.9815\tacc=0.2771\t551.9 examples/second\n",
      "[Step=1650]\tLoss=1.9745\tacc=0.2740\t1438.1 examples/second\n",
      "[Step=1700]\tLoss=1.9763\tacc=0.2722\t1498.4 examples/second\n",
      "[Step=1750]\tLoss=1.9708\tacc=0.2742\t1679.6 examples/second\n",
      "Test Loss=1.9956, Test acc=0.2668\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=1.9484\tacc=0.2890\t730.4 examples/second\n",
      "[Step=1850]\tLoss=1.9521\tacc=0.2820\t1598.0 examples/second\n",
      "[Step=1900]\tLoss=1.9505\tacc=0.2826\t1450.1 examples/second\n",
      "[Step=1950]\tLoss=1.9485\tacc=0.2815\t1996.6 examples/second\n",
      "Test Loss=1.9802, Test acc=0.2590\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=1.9416\tacc=0.2793\t732.9 examples/second\n",
      "[Step=2050]\tLoss=1.9378\tacc=0.2812\t1614.9 examples/second\n",
      "[Step=2100]\tLoss=1.9351\tacc=0.2847\t1488.7 examples/second\n",
      "[Step=2150]\tLoss=1.9291\tacc=0.2863\t2035.3 examples/second\n",
      "Test Loss=1.9814, Test acc=0.2681\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=1.9174\tacc=0.2874\t730.7 examples/second\n",
      "[Step=2250]\tLoss=1.9099\tacc=0.2914\t1519.6 examples/second\n",
      "[Step=2300]\tLoss=1.9080\tacc=0.2932\t1445.1 examples/second\n",
      "[Step=2350]\tLoss=1.9063\tacc=0.2929\t1754.9 examples/second\n",
      "Test Loss=1.8843, Test acc=0.3172\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=1.9028\tacc=0.2957\t690.2 examples/second\n",
      "[Step=2450]\tLoss=1.8943\tacc=0.2970\t1512.9 examples/second\n",
      "[Step=2500]\tLoss=1.8879\tacc=0.2986\t1501.8 examples/second\n",
      "Test Loss=1.9390, Test acc=0.2707\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=1.8633\tacc=0.3027\t744.7 examples/second\n",
      "[Step=2600]\tLoss=1.8664\tacc=0.3004\t1610.5 examples/second\n",
      "[Step=2650]\tLoss=1.8635\tacc=0.3067\t1543.8 examples/second\n",
      "[Step=2700]\tLoss=1.8568\tacc=0.3070\t1439.8 examples/second\n",
      "Test Loss=1.8660, Test acc=0.2838\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=1.8372\tacc=0.3171\t766.1 examples/second\n",
      "[Step=2800]\tLoss=1.8265\tacc=0.3165\t1558.9 examples/second\n",
      "[Step=2850]\tLoss=1.8272\tacc=0.3156\t1564.3 examples/second\n",
      "[Step=2900]\tLoss=1.8270\tacc=0.3159\t1413.3 examples/second\n",
      "Test Loss=1.9261, Test acc=0.2735\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=1.8358\tacc=0.2949\t773.9 examples/second\n",
      "[Step=3000]\tLoss=1.8195\tacc=0.3155\t1521.3 examples/second\n",
      "[Step=3050]\tLoss=1.8090\tacc=0.3210\t1563.9 examples/second\n",
      "[Step=3100]\tLoss=1.8067\tacc=0.3229\t1525.9 examples/second\n",
      "Test Loss=1.8102, Test acc=0.3174\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=1.7710\tacc=0.3343\t815.1 examples/second\n",
      "[Step=3200]\tLoss=1.7840\tacc=0.3355\t1632.9 examples/second\n",
      "[Step=3250]\tLoss=1.7836\tacc=0.3334\t1485.0 examples/second\n",
      "[Step=3300]\tLoss=1.7824\tacc=0.3333\t1536.0 examples/second\n",
      "Test Loss=1.8970, Test acc=0.2962\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=1.7753\tacc=0.3372\t764.0 examples/second\n",
      "[Step=3400]\tLoss=1.7707\tacc=0.3391\t1582.1 examples/second\n",
      "[Step=3450]\tLoss=1.7681\tacc=0.3361\t1487.3 examples/second\n",
      "[Step=3500]\tLoss=1.7682\tacc=0.3366\t1556.9 examples/second\n",
      "Test Loss=1.7980, Test acc=0.3119\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=1.7385\tacc=0.3441\t807.3 examples/second\n",
      "[Step=3600]\tLoss=1.7419\tacc=0.3480\t1561.6 examples/second\n",
      "[Step=3650]\tLoss=1.7440\tacc=0.3488\t1597.6 examples/second\n",
      "[Step=3700]\tLoss=1.7472\tacc=0.3475\t1559.1 examples/second\n",
      "Test Loss=1.8974, Test acc=0.3159\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=1.7439\tacc=0.3376\t739.9 examples/second\n",
      "[Step=3800]\tLoss=1.7336\tacc=0.3464\t1469.4 examples/second\n",
      "[Step=3850]\tLoss=1.7342\tacc=0.3460\t1573.5 examples/second\n",
      "[Step=3900]\tLoss=1.7361\tacc=0.3466\t1608.4 examples/second\n",
      "Test Loss=1.7782, Test acc=0.3190\n",
      "Saving...\n",
      "Files already downloaded and verified\n",
      "Test Loss=1.7782, Test accuracy=0.3190\n",
      "-------------------- Nbits =  3 --------------------------------------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Test Loss=3.7940, Test accuracy=0.0891\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=2.0623\tacc=0.3746\t1665.8 examples/second\n",
      "[Step=100]\tLoss=1.6799\tacc=0.4548\t1476.6 examples/second\n",
      "[Step=150]\tLoss=1.4775\tacc=0.5109\t1494.7 examples/second\n",
      "Test Loss=0.8352, Test acc=0.7067\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.8344\tacc=0.7227\t751.0 examples/second\n",
      "[Step=250]\tLoss=0.7898\tacc=0.7277\t1585.1 examples/second\n",
      "[Step=300]\tLoss=0.7511\tacc=0.7418\t1437.2 examples/second\n",
      "[Step=350]\tLoss=0.7154\tacc=0.7535\t1464.5 examples/second\n",
      "Test Loss=0.6249, Test acc=0.7873\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.5903\tacc=0.7959\t761.1 examples/second\n",
      "[Step=450]\tLoss=0.5961\tacc=0.7978\t1592.4 examples/second\n",
      "[Step=500]\tLoss=0.5738\tacc=0.8039\t1407.4 examples/second\n",
      "[Step=550]\tLoss=0.5582\tacc=0.8089\t1551.7 examples/second\n",
      "Test Loss=0.5586, Test acc=0.8116\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.5041\tacc=0.8262\t801.0 examples/second\n",
      "[Step=650]\tLoss=0.5072\tacc=0.8233\t1442.2 examples/second\n",
      "[Step=700]\tLoss=0.4971\tacc=0.8284\t1524.7 examples/second\n",
      "[Step=750]\tLoss=0.4961\tacc=0.8295\t1551.0 examples/second\n",
      "Test Loss=0.5301, Test acc=0.8204\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.4651\tacc=0.8345\t807.4 examples/second\n",
      "[Step=850]\tLoss=0.4493\tacc=0.8420\t1572.6 examples/second\n",
      "[Step=900]\tLoss=0.4503\tacc=0.8431\t1436.1 examples/second\n",
      "[Step=950]\tLoss=0.4483\tacc=0.8449\t1481.8 examples/second\n",
      "Test Loss=0.5097, Test acc=0.8296\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.4474\tacc=0.8471\t746.4 examples/second\n",
      "[Step=1050]\tLoss=0.4285\tacc=0.8513\t1511.2 examples/second\n",
      "[Step=1100]\tLoss=0.4213\tacc=0.8537\t1612.1 examples/second\n",
      "[Step=1150]\tLoss=0.4137\tacc=0.8558\t1513.2 examples/second\n",
      "Test Loss=0.4833, Test acc=0.8376\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.4012\tacc=0.8607\t787.8 examples/second\n",
      "[Step=1250]\tLoss=0.3943\tacc=0.8637\t1538.0 examples/second\n",
      "[Step=1300]\tLoss=0.3945\tacc=0.8643\t1601.9 examples/second\n",
      "[Step=1350]\tLoss=0.3947\tacc=0.8633\t1802.7 examples/second\n",
      "Test Loss=0.4601, Test acc=0.8490\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.3829\tacc=0.8673\t740.1 examples/second\n",
      "[Step=1450]\tLoss=0.3780\tacc=0.8698\t1528.7 examples/second\n",
      "[Step=1500]\tLoss=0.3781\tacc=0.8699\t1458.5 examples/second\n",
      "[Step=1550]\tLoss=0.3757\tacc=0.8699\t1835.7 examples/second\n",
      "Test Loss=0.4671, Test acc=0.8458\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.3511\tacc=0.8771\t727.7 examples/second\n",
      "[Step=1650]\tLoss=0.3512\tacc=0.8776\t1526.8 examples/second\n",
      "[Step=1700]\tLoss=0.3548\tacc=0.8761\t1559.6 examples/second\n",
      "[Step=1750]\tLoss=0.3562\tacc=0.8758\t1751.3 examples/second\n",
      "Test Loss=0.4361, Test acc=0.8535\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.3423\tacc=0.8798\t753.3 examples/second\n",
      "[Step=1850]\tLoss=0.3380\tacc=0.8817\t1541.7 examples/second\n",
      "[Step=1900]\tLoss=0.3345\tacc=0.8834\t1528.8 examples/second\n",
      "[Step=1950]\tLoss=0.3348\tacc=0.8830\t1859.6 examples/second\n",
      "Test Loss=0.4526, Test acc=0.8514\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.3327\tacc=0.8830\t710.4 examples/second\n",
      "[Step=2050]\tLoss=0.3337\tacc=0.8832\t1659.2 examples/second\n",
      "[Step=2100]\tLoss=0.3327\tacc=0.8839\t1466.2 examples/second\n",
      "[Step=2150]\tLoss=0.3329\tacc=0.8836\t2041.9 examples/second\n",
      "Test Loss=0.4366, Test acc=0.8563\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.3147\tacc=0.8881\t710.8 examples/second\n",
      "[Step=2250]\tLoss=0.3146\tacc=0.8907\t1503.8 examples/second\n",
      "[Step=2300]\tLoss=0.3183\tacc=0.8888\t1431.7 examples/second\n",
      "[Step=2350]\tLoss=0.3196\tacc=0.8885\t2110.2 examples/second\n",
      "Test Loss=0.4338, Test acc=0.8570\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.3089\tacc=0.8925\t704.9 examples/second\n",
      "[Step=2450]\tLoss=0.3131\tacc=0.8919\t1528.7 examples/second\n",
      "[Step=2500]\tLoss=0.3089\tacc=0.8938\t1569.2 examples/second\n",
      "Test Loss=0.4328, Test acc=0.8586\n",
      "Saving...\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.2823\tacc=0.8867\t751.0 examples/second\n",
      "[Step=2600]\tLoss=0.3075\tacc=0.8924\t1620.3 examples/second\n",
      "[Step=2650]\tLoss=0.3077\tacc=0.8921\t1587.1 examples/second\n",
      "[Step=2700]\tLoss=0.3038\tacc=0.8945\t1499.7 examples/second\n",
      "Test Loss=0.4051, Test acc=0.8668\n",
      "Saving...\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.3117\tacc=0.8880\t764.7 examples/second\n",
      "[Step=2800]\tLoss=0.2971\tacc=0.8940\t1623.1 examples/second\n",
      "[Step=2850]\tLoss=0.2983\tacc=0.8943\t1607.0 examples/second\n",
      "[Step=2900]\tLoss=0.2939\tacc=0.8971\t1466.3 examples/second\n",
      "Test Loss=0.4066, Test acc=0.8661\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.2955\tacc=0.8969\t810.4 examples/second\n",
      "[Step=3000]\tLoss=0.2869\tacc=0.8993\t1671.4 examples/second\n",
      "[Step=3050]\tLoss=0.2847\tacc=0.9002\t1422.7 examples/second\n",
      "[Step=3100]\tLoss=0.2840\tacc=0.8997\t1528.1 examples/second\n",
      "Test Loss=0.4053, Test acc=0.8668\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.2758\tacc=0.9065\t782.2 examples/second\n",
      "[Step=3200]\tLoss=0.2800\tacc=0.9019\t1497.8 examples/second\n",
      "[Step=3250]\tLoss=0.2834\tacc=0.8998\t1489.2 examples/second\n",
      "[Step=3300]\tLoss=0.2830\tacc=0.9004\t1524.7 examples/second\n",
      "Test Loss=0.3910, Test acc=0.8697\n",
      "Saving...\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.2707\tacc=0.9052\t742.1 examples/second\n",
      "[Step=3400]\tLoss=0.2726\tacc=0.9060\t1620.2 examples/second\n",
      "[Step=3450]\tLoss=0.2744\tacc=0.9041\t1482.6 examples/second\n",
      "[Step=3500]\tLoss=0.2757\tacc=0.9035\t1536.0 examples/second\n",
      "Test Loss=0.4034, Test acc=0.8671\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.2599\tacc=0.9126\t750.8 examples/second\n",
      "[Step=3600]\tLoss=0.2676\tacc=0.9070\t1364.0 examples/second\n",
      "[Step=3650]\tLoss=0.2685\tacc=0.9063\t1450.5 examples/second\n",
      "[Step=3700]\tLoss=0.2673\tacc=0.9072\t1642.4 examples/second\n",
      "Test Loss=0.3937, Test acc=0.8695\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.2668\tacc=0.9055\t745.8 examples/second\n",
      "[Step=3800]\tLoss=0.2670\tacc=0.9059\t1530.3 examples/second\n",
      "[Step=3850]\tLoss=0.2673\tacc=0.9049\t1582.0 examples/second\n",
      "[Step=3900]\tLoss=0.2683\tacc=0.9049\t1546.6 examples/second\n",
      "Test Loss=0.4067, Test acc=0.8682\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3910, Test accuracy=0.8697\n",
      "-------------------- Nbits =  4 --------------------------------------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Test Loss=2.3464, Test accuracy=0.1000\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.9768\tacc=0.7045\t1589.2 examples/second\n",
      "[Step=100]\tLoss=0.7150\tacc=0.7784\t1550.1 examples/second\n",
      "[Step=150]\tLoss=0.6047\tacc=0.8095\t1470.2 examples/second\n",
      "Test Loss=0.4564, Test acc=0.8451\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.4119\tacc=0.8672\t750.0 examples/second\n",
      "[Step=250]\tLoss=0.3618\tacc=0.8801\t1654.4 examples/second\n",
      "[Step=300]\tLoss=0.3498\tacc=0.8854\t1546.6 examples/second\n",
      "[Step=350]\tLoss=0.3469\tacc=0.8860\t1454.6 examples/second\n",
      "Test Loss=0.4265, Test acc=0.8586\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.3326\tacc=0.8848\t756.6 examples/second\n",
      "[Step=450]\tLoss=0.3287\tacc=0.8905\t1598.2 examples/second\n",
      "[Step=500]\tLoss=0.3223\tacc=0.8933\t1457.9 examples/second\n",
      "[Step=550]\tLoss=0.3164\tacc=0.8940\t1584.8 examples/second\n",
      "Test Loss=0.4048, Test acc=0.8664\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.2899\tacc=0.9023\t756.4 examples/second\n",
      "[Step=650]\tLoss=0.2882\tacc=0.9023\t1598.2 examples/second\n",
      "[Step=700]\tLoss=0.2847\tacc=0.9044\t1499.5 examples/second\n",
      "[Step=750]\tLoss=0.2844\tacc=0.9036\t1593.2 examples/second\n",
      "Test Loss=0.3832, Test acc=0.8728\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.2604\tacc=0.9133\t771.4 examples/second\n",
      "[Step=850]\tLoss=0.2638\tacc=0.9105\t1454.7 examples/second\n",
      "[Step=900]\tLoss=0.2615\tacc=0.9110\t1509.2 examples/second\n",
      "[Step=950]\tLoss=0.2598\tacc=0.9115\t1463.5 examples/second\n",
      "Test Loss=0.3736, Test acc=0.8767\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.2448\tacc=0.9137\t731.7 examples/second\n",
      "[Step=1050]\tLoss=0.2447\tacc=0.9150\t1552.8 examples/second\n",
      "[Step=1100]\tLoss=0.2415\tacc=0.9158\t1535.8 examples/second\n",
      "[Step=1150]\tLoss=0.2401\tacc=0.9160\t1544.3 examples/second\n",
      "Test Loss=0.3686, Test acc=0.8793\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.2318\tacc=0.9196\t764.9 examples/second\n",
      "[Step=1250]\tLoss=0.2307\tacc=0.9195\t1610.3 examples/second\n",
      "[Step=1300]\tLoss=0.2297\tacc=0.9210\t1540.3 examples/second\n",
      "[Step=1350]\tLoss=0.2297\tacc=0.9203\t1441.7 examples/second\n",
      "Test Loss=0.3611, Test acc=0.8829\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.2158\tacc=0.9259\t725.3 examples/second\n",
      "[Step=1450]\tLoss=0.2208\tacc=0.9231\t1605.3 examples/second\n",
      "[Step=1500]\tLoss=0.2209\tacc=0.9229\t1412.5 examples/second\n",
      "[Step=1550]\tLoss=0.2204\tacc=0.9233\t1723.5 examples/second\n",
      "Test Loss=0.3549, Test acc=0.8856\n",
      "Saving...\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.2042\tacc=0.9290\t710.3 examples/second\n",
      "[Step=1650]\tLoss=0.2096\tacc=0.9258\t1511.5 examples/second\n",
      "[Step=1700]\tLoss=0.2098\tacc=0.9263\t1340.3 examples/second\n",
      "[Step=1750]\tLoss=0.2111\tacc=0.9264\t1720.1 examples/second\n",
      "Test Loss=0.3506, Test acc=0.8874\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.2085\tacc=0.9280\t715.7 examples/second\n",
      "[Step=1850]\tLoss=0.2089\tacc=0.9263\t1535.6 examples/second\n",
      "[Step=1900]\tLoss=0.2039\tacc=0.9280\t1378.3 examples/second\n",
      "[Step=1950]\tLoss=0.2048\tacc=0.9283\t1840.4 examples/second\n",
      "Test Loss=0.3572, Test acc=0.8851\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.1928\tacc=0.9323\t688.9 examples/second\n",
      "[Step=2050]\tLoss=0.1996\tacc=0.9313\t1520.1 examples/second\n",
      "[Step=2100]\tLoss=0.1983\tacc=0.9308\t1385.6 examples/second\n",
      "[Step=2150]\tLoss=0.1965\tacc=0.9316\t2054.6 examples/second\n",
      "Test Loss=0.3549, Test acc=0.8892\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.1934\tacc=0.9324\t690.6 examples/second\n",
      "[Step=2250]\tLoss=0.1953\tacc=0.9305\t1480.9 examples/second\n",
      "[Step=2300]\tLoss=0.1933\tacc=0.9320\t1481.5 examples/second\n",
      "[Step=2350]\tLoss=0.1923\tacc=0.9327\t1868.4 examples/second\n",
      "Test Loss=0.3418, Test acc=0.8921\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.1904\tacc=0.9330\t697.3 examples/second\n",
      "[Step=2450]\tLoss=0.1886\tacc=0.9334\t1483.0 examples/second\n",
      "[Step=2500]\tLoss=0.1887\tacc=0.9326\t1496.4 examples/second\n",
      "Test Loss=0.3485, Test acc=0.8923\n",
      "Saving...\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.1737\tacc=0.9434\t718.5 examples/second\n",
      "[Step=2600]\tLoss=0.1865\tacc=0.9339\t1417.5 examples/second\n",
      "[Step=2650]\tLoss=0.1826\tacc=0.9356\t1436.3 examples/second\n",
      "[Step=2700]\tLoss=0.1834\tacc=0.9353\t1546.1 examples/second\n",
      "Test Loss=0.3425, Test acc=0.8977\n",
      "Saving...\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.2039\tacc=0.9303\t753.4 examples/second\n",
      "[Step=2800]\tLoss=0.1837\tacc=0.9372\t1474.5 examples/second\n",
      "[Step=2850]\tLoss=0.1764\tacc=0.9385\t1495.4 examples/second\n",
      "[Step=2900]\tLoss=0.1765\tacc=0.9384\t1430.5 examples/second\n",
      "Test Loss=0.3425, Test acc=0.8950\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.1737\tacc=0.9395\t761.4 examples/second\n",
      "[Step=3000]\tLoss=0.1712\tacc=0.9410\t1441.6 examples/second\n",
      "[Step=3050]\tLoss=0.1692\tacc=0.9420\t1518.8 examples/second\n",
      "[Step=3100]\tLoss=0.1690\tacc=0.9419\t1499.8 examples/second\n",
      "Test Loss=0.3345, Test acc=0.8963\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.1626\tacc=0.9464\t762.3 examples/second\n",
      "[Step=3200]\tLoss=0.1670\tacc=0.9415\t1493.2 examples/second\n",
      "[Step=3250]\tLoss=0.1659\tacc=0.9418\t1510.2 examples/second\n",
      "[Step=3300]\tLoss=0.1659\tacc=0.9416\t1384.8 examples/second\n",
      "Test Loss=0.3419, Test acc=0.8965\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.1538\tacc=0.9434\t768.9 examples/second\n",
      "[Step=3400]\tLoss=0.1583\tacc=0.9432\t1634.9 examples/second\n",
      "[Step=3450]\tLoss=0.1590\tacc=0.9428\t1449.2 examples/second\n",
      "[Step=3500]\tLoss=0.1593\tacc=0.9427\t1607.3 examples/second\n",
      "Test Loss=0.3409, Test acc=0.8966\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.1502\tacc=0.9474\t768.9 examples/second\n",
      "[Step=3600]\tLoss=0.1548\tacc=0.9474\t1612.6 examples/second\n",
      "[Step=3650]\tLoss=0.1557\tacc=0.9461\t1441.3 examples/second\n",
      "[Step=3700]\tLoss=0.1582\tacc=0.9455\t1700.9 examples/second\n",
      "Test Loss=0.3387, Test acc=0.8960\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.1530\tacc=0.9423\t768.0 examples/second\n",
      "[Step=3800]\tLoss=0.1539\tacc=0.9451\t1469.1 examples/second\n",
      "[Step=3850]\tLoss=0.1554\tacc=0.9454\t1462.7 examples/second\n",
      "[Step=3900]\tLoss=0.1571\tacc=0.9444\t1652.2 examples/second\n",
      "Test Loss=0.3294, Test acc=0.8987\n",
      "Saving...\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3294, Test accuracy=0.8987\n"
     ]
    }
   ],
   "source": [
    "for Nbits in range(2,5):\n",
    "    print(\"-\"*20,\"Nbits = \",Nbits,\"-\"*80)\n",
    "\n",
    "\n",
    "    net = ResNetCIFAR(num_layers=20, Nbits=Nbits)\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(\"net_after_global_iterative_prune.pt\"))\n",
    "    test(net)\n",
    "    # Quantized model finetuning\n",
    "    finetune(net, epochs=20, batch_size=256, lr=0.002, reg=1e-4)\n",
    "\n",
    "    # Load the model with best accuracy\n",
    "    net.load_state_dict(torch.load(\"quantized_net_after_finetune.pt\"))\n",
    "    test(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab3 (d) Quantize pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3474, Test accuracy=0.8844\n"
     ]
    }
   ],
   "source": [
    "# Define quantized model and load weight\n",
    "Nbits = 6 #Change this value to finish (d)\n",
    "\n",
    "net = ResNetCIFAR(num_layers=20, Nbits=Nbits)\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(\"net_after_global_iterative_prune.pt\"))\n",
    "test(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5c9b6cc678ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Quantized model finetuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfinetune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the model with best accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantized_net_after_finetune.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/hw4/train_util.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(net, epochs, batch_size, lr, reg, log_every_n)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Quantized model finetuning\n",
    "finetune(net, epochs=20, batch_size=256, lr=0.002, reg=1e-4)\n",
    "\n",
    "# Load the model with best accuracy\n",
    "net.load_state_dict(torch.load(\"quantized_net_after_finetune.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Nbits =  2 --------------------------------------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Test Loss=1530.5690, Test accuracy=0.1000\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=2.2840\tacc=0.1534\t1268.5 examples/second\n",
      "[Step=100]\tLoss=2.2402\tacc=0.1596\t1137.2 examples/second\n",
      "[Step=150]\tLoss=2.2143\tacc=0.1647\t1195.1 examples/second\n",
      "Test Loss=2.1660, Test acc=0.1815\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=2.1573\tacc=0.1855\t508.1 examples/second\n",
      "[Step=250]\tLoss=2.1320\tacc=0.1934\t1274.9 examples/second\n",
      "[Step=300]\tLoss=2.1276\tacc=0.1966\t1174.6 examples/second\n",
      "[Step=350]\tLoss=2.1227\tacc=0.2019\t1156.0 examples/second\n",
      "Test Loss=2.1152, Test acc=0.2104\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=2.1142\tacc=0.2114\t481.8 examples/second\n",
      "[Step=450]\tLoss=2.0971\tacc=0.2214\t1129.2 examples/second\n",
      "[Step=500]\tLoss=2.0952\tacc=0.2260\t1234.2 examples/second\n",
      "[Step=550]\tLoss=2.0917\tacc=0.2281\t1208.2 examples/second\n",
      "Test Loss=2.0922, Test acc=0.2333\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=2.0715\tacc=0.2389\t486.3 examples/second\n",
      "[Step=650]\tLoss=2.0790\tacc=0.2388\t1250.4 examples/second\n",
      "[Step=700]\tLoss=2.0759\tacc=0.2425\t1152.4 examples/second\n",
      "[Step=750]\tLoss=2.0712\tacc=0.2414\t1080.3 examples/second\n",
      "Test Loss=2.0600, Test acc=0.2417\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=2.0321\tacc=0.2556\t471.1 examples/second\n",
      "[Step=850]\tLoss=2.0480\tacc=0.2501\t1219.1 examples/second\n",
      "[Step=900]\tLoss=2.0478\tacc=0.2512\t1132.7 examples/second\n",
      "[Step=950]\tLoss=2.0446\tacc=0.2513\t1135.1 examples/second\n",
      "Test Loss=2.0438, Test acc=0.2505\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=2.0251\tacc=0.2467\t506.5 examples/second\n",
      "[Step=1050]\tLoss=2.0286\tacc=0.2546\t1184.0 examples/second\n",
      "[Step=1100]\tLoss=2.0251\tacc=0.2550\t1019.8 examples/second\n",
      "[Step=1150]\tLoss=2.0235\tacc=0.2574\t1094.0 examples/second\n",
      "Test Loss=2.0486, Test acc=0.2461\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=2.0260\tacc=0.2624\t490.4 examples/second\n",
      "[Step=1250]\tLoss=2.0217\tacc=0.2627\t1120.0 examples/second\n",
      "[Step=1300]\tLoss=2.0139\tacc=0.2642\t1203.0 examples/second\n",
      "[Step=1350]\tLoss=2.0077\tacc=0.2644\t1119.6 examples/second\n",
      "Test Loss=2.0174, Test acc=0.2656\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=1.9880\tacc=0.2783\t514.0 examples/second\n",
      "[Step=1450]\tLoss=1.9881\tacc=0.2744\t1131.2 examples/second\n",
      "[Step=1500]\tLoss=1.9881\tacc=0.2738\t1070.0 examples/second\n",
      "[Step=1550]\tLoss=1.9829\tacc=0.2744\t1240.3 examples/second\n",
      "Test Loss=1.9860, Test acc=0.2710\n",
      "Saving...\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=1.9631\tacc=0.2877\t480.8 examples/second\n",
      "[Step=1650]\tLoss=1.9668\tacc=0.2828\t1186.0 examples/second\n",
      "[Step=1700]\tLoss=1.9676\tacc=0.2793\t1008.5 examples/second\n",
      "[Step=1750]\tLoss=1.9682\tacc=0.2793\t1222.0 examples/second\n",
      "Test Loss=1.9614, Test acc=0.2805\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=1.9571\tacc=0.2849\t499.4 examples/second\n",
      "[Step=1850]\tLoss=1.9568\tacc=0.2834\t1256.9 examples/second\n",
      "[Step=1900]\tLoss=1.9500\tacc=0.2850\t1141.2 examples/second\n",
      "[Step=1950]\tLoss=1.9481\tacc=0.2850\t1383.5 examples/second\n",
      "Test Loss=1.9523, Test acc=0.2781\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=1.9380\tacc=0.2866\t537.8 examples/second\n",
      "[Step=2050]\tLoss=1.9359\tacc=0.2887\t1179.5 examples/second\n",
      "[Step=2100]\tLoss=1.9353\tacc=0.2853\t1073.2 examples/second\n",
      "[Step=2150]\tLoss=1.9325\tacc=0.2857\t1272.8 examples/second\n",
      "Test Loss=1.9413, Test acc=0.2830\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=1.9004\tacc=0.2963\t515.7 examples/second\n",
      "[Step=2250]\tLoss=1.9082\tacc=0.2926\t1218.3 examples/second\n",
      "[Step=2300]\tLoss=1.9084\tacc=0.2932\t1036.6 examples/second\n",
      "[Step=2350]\tLoss=1.9073\tacc=0.2938\t1249.3 examples/second\n",
      "Test Loss=1.9943, Test acc=0.2603\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=1.8999\tacc=0.3030\t466.3 examples/second\n",
      "[Step=2450]\tLoss=1.8940\tacc=0.2993\t1154.5 examples/second\n",
      "[Step=2500]\tLoss=1.8869\tacc=0.3004\t1059.6 examples/second\n",
      "Test Loss=1.8908, Test acc=0.2875\n",
      "Saving...\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=1.8774\tacc=0.2773\t471.3 examples/second\n",
      "[Step=2600]\tLoss=1.8701\tacc=0.2989\t1269.0 examples/second\n",
      "[Step=2650]\tLoss=1.8628\tacc=0.3081\t1123.9 examples/second\n",
      "[Step=2700]\tLoss=1.8596\tacc=0.3065\t1074.3 examples/second\n",
      "Test Loss=2.0652, Test acc=0.2477\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=1.7934\tacc=0.3340\t481.1 examples/second\n",
      "[Step=2800]\tLoss=1.8373\tacc=0.3142\t1247.1 examples/second\n",
      "[Step=2850]\tLoss=1.8365\tacc=0.3146\t1184.7 examples/second\n",
      "[Step=2900]\tLoss=1.8365\tacc=0.3145\t1146.5 examples/second\n",
      "Test Loss=1.8600, Test acc=0.3036\n",
      "Saving...\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=1.8042\tacc=0.3141\t481.0 examples/second\n",
      "[Step=3000]\tLoss=1.8058\tacc=0.3178\t1200.8 examples/second\n",
      "[Step=3050]\tLoss=1.8070\tacc=0.3225\t1076.6 examples/second\n",
      "[Step=3100]\tLoss=1.8065\tacc=0.3246\t1126.5 examples/second\n",
      "Test Loss=1.9755, Test acc=0.2739\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=1.7979\tacc=0.3209\t497.6 examples/second\n",
      "[Step=3200]\tLoss=1.7946\tacc=0.3237\t1310.7 examples/second\n",
      "[Step=3250]\tLoss=1.7899\tacc=0.3267\t1093.4 examples/second\n",
      "[Step=3300]\tLoss=1.7874\tacc=0.3285\t1158.5 examples/second\n",
      "Test Loss=1.8912, Test acc=0.3001\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=1.7819\tacc=0.3262\t505.4 examples/second\n",
      "[Step=3400]\tLoss=1.7666\tacc=0.3393\t1239.1 examples/second\n",
      "[Step=3450]\tLoss=1.7648\tacc=0.3400\t1057.8 examples/second\n",
      "[Step=3500]\tLoss=1.7637\tacc=0.3398\t1219.6 examples/second\n",
      "Test Loss=1.9007, Test acc=0.3027\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=1.7650\tacc=0.3450\t499.5 examples/second\n",
      "[Step=3600]\tLoss=1.7529\tacc=0.3408\t1264.5 examples/second\n",
      "[Step=3650]\tLoss=1.7490\tacc=0.3465\t1036.5 examples/second\n",
      "[Step=3700]\tLoss=1.7483\tacc=0.3461\t1162.1 examples/second\n",
      "Test Loss=2.0753, Test acc=0.2650\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=1.7174\tacc=0.3565\t492.7 examples/second\n",
      "[Step=3800]\tLoss=1.7286\tacc=0.3491\t1242.0 examples/second\n",
      "[Step=3850]\tLoss=1.7290\tacc=0.3472\t1108.1 examples/second\n",
      "[Step=3900]\tLoss=1.7302\tacc=0.3478\t1167.0 examples/second\n",
      "Test Loss=1.7578, Test acc=0.3339\n",
      "Saving...\n",
      "Files already downloaded and verified\n",
      "Test Loss=1.7578, Test accuracy=0.3339\n",
      "-------------------- Nbits =  3 --------------------------------------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Test Loss=1.1386, Test accuracy=0.6585\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=1.9856\tacc=0.4102\t1189.2 examples/second\n",
      "[Step=100]\tLoss=1.6105\tacc=0.4836\t1033.1 examples/second\n",
      "[Step=150]\tLoss=1.4156\tacc=0.5340\t1009.6 examples/second\n",
      "Test Loss=0.8388, Test acc=0.7093\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.8423\tacc=0.7139\t504.3 examples/second\n",
      "[Step=250]\tLoss=0.7884\tacc=0.7287\t1219.9 examples/second\n",
      "[Step=300]\tLoss=0.7460\tacc=0.7439\t1084.0 examples/second\n",
      "[Step=350]\tLoss=0.7125\tacc=0.7555\t1121.9 examples/second\n",
      "Test Loss=0.6329, Test acc=0.7834\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.5917\tacc=0.7959\t497.2 examples/second\n",
      "[Step=450]\tLoss=0.5761\tacc=0.8025\t1142.9 examples/second\n",
      "[Step=500]\tLoss=0.5574\tacc=0.8088\t1051.1 examples/second\n",
      "[Step=550]\tLoss=0.5456\tacc=0.8123\t972.2 examples/second\n",
      "Test Loss=0.5607, Test acc=0.8142\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.4962\tacc=0.8285\t487.8 examples/second\n",
      "[Step=650]\tLoss=0.4958\tacc=0.8283\t1153.0 examples/second\n",
      "[Step=700]\tLoss=0.4944\tacc=0.8296\t1136.2 examples/second\n",
      "[Step=750]\tLoss=0.4819\tacc=0.8345\t1029.2 examples/second\n",
      "Test Loss=0.5294, Test acc=0.8239\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.4618\tacc=0.8433\t491.2 examples/second\n",
      "[Step=850]\tLoss=0.4428\tacc=0.8478\t1148.2 examples/second\n",
      "[Step=900]\tLoss=0.4417\tacc=0.8486\t1115.2 examples/second\n",
      "[Step=950]\tLoss=0.4378\tacc=0.8488\t1125.8 examples/second\n",
      "Test Loss=0.4970, Test acc=0.8350\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.4199\tacc=0.8578\t509.9 examples/second\n",
      "[Step=1050]\tLoss=0.4145\tacc=0.8568\t1206.9 examples/second\n",
      "[Step=1100]\tLoss=0.4073\tacc=0.8580\t1177.0 examples/second\n",
      "[Step=1150]\tLoss=0.4046\tacc=0.8588\t1100.7 examples/second\n",
      "Test Loss=0.4711, Test acc=0.8445\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.3785\tacc=0.8667\t493.1 examples/second\n",
      "[Step=1250]\tLoss=0.3895\tacc=0.8639\t1168.7 examples/second\n",
      "[Step=1300]\tLoss=0.3866\tacc=0.8653\t1113.5 examples/second\n",
      "[Step=1350]\tLoss=0.3855\tacc=0.8655\t1223.2 examples/second\n",
      "Test Loss=0.4862, Test acc=0.8343\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.3702\tacc=0.8726\t503.2 examples/second\n",
      "[Step=1450]\tLoss=0.3646\tacc=0.8756\t1123.0 examples/second\n",
      "[Step=1500]\tLoss=0.3637\tacc=0.8762\t976.9 examples/second\n",
      "[Step=1550]\tLoss=0.3616\tacc=0.8763\t1012.7 examples/second\n",
      "Test Loss=0.4528, Test acc=0.8479\n",
      "Saving...\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.3519\tacc=0.8754\t490.1 examples/second\n",
      "[Step=1650]\tLoss=0.3541\tacc=0.8758\t1247.0 examples/second\n",
      "[Step=1700]\tLoss=0.3527\tacc=0.8772\t1068.7 examples/second\n",
      "[Step=1750]\tLoss=0.3520\tacc=0.8774\t1185.5 examples/second\n",
      "Test Loss=0.4336, Test acc=0.8560\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.3453\tacc=0.8798\t493.0 examples/second\n",
      "[Step=1850]\tLoss=0.3376\tacc=0.8825\t1163.1 examples/second\n",
      "[Step=1900]\tLoss=0.3391\tacc=0.8828\t1017.5 examples/second\n",
      "[Step=1950]\tLoss=0.3399\tacc=0.8827\t1193.5 examples/second\n",
      "Test Loss=0.4318, Test acc=0.8583\n",
      "Saving...\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.3203\tacc=0.8881\t467.5 examples/second\n",
      "[Step=2050]\tLoss=0.3266\tacc=0.8871\t1141.6 examples/second\n",
      "[Step=2100]\tLoss=0.3253\tacc=0.8876\t1075.6 examples/second\n",
      "[Step=2150]\tLoss=0.3229\tacc=0.8872\t1199.9 examples/second\n",
      "Test Loss=0.4361, Test acc=0.8589\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.3203\tacc=0.8865\t459.2 examples/second\n",
      "[Step=2250]\tLoss=0.3174\tacc=0.8889\t1162.7 examples/second\n",
      "[Step=2300]\tLoss=0.3121\tacc=0.8905\t1036.7 examples/second\n",
      "[Step=2350]\tLoss=0.3130\tacc=0.8903\t1133.7 examples/second\n",
      "Test Loss=0.4273, Test acc=0.8600\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.2990\tacc=0.8956\t494.5 examples/second\n",
      "[Step=2450]\tLoss=0.3080\tacc=0.8918\t1139.8 examples/second\n",
      "[Step=2500]\tLoss=0.3105\tacc=0.8922\t1055.4 examples/second\n",
      "Test Loss=0.4213, Test acc=0.8616\n",
      "Saving...\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.2666\tacc=0.9062\t447.1 examples/second\n",
      "[Step=2600]\tLoss=0.3006\tacc=0.8927\t1081.5 examples/second\n",
      "[Step=2650]\tLoss=0.3017\tacc=0.8941\t1117.5 examples/second\n",
      "[Step=2700]\tLoss=0.2999\tacc=0.8951\t1241.5 examples/second\n",
      "Test Loss=0.4251, Test acc=0.8633\n",
      "Saving...\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.3157\tacc=0.8880\t445.4 examples/second\n",
      "[Step=2800]\tLoss=0.2911\tacc=0.8954\t1151.5 examples/second\n",
      "[Step=2850]\tLoss=0.2934\tacc=0.8961\t1085.5 examples/second\n",
      "[Step=2900]\tLoss=0.2920\tacc=0.8973\t1029.1 examples/second\n",
      "Test Loss=0.4089, Test acc=0.8683\n",
      "Saving...\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.2869\tacc=0.8930\t437.4 examples/second\n",
      "[Step=3000]\tLoss=0.2769\tacc=0.9008\t1269.8 examples/second\n",
      "[Step=3050]\tLoss=0.2825\tacc=0.9007\t1158.2 examples/second\n",
      "[Step=3100]\tLoss=0.2826\tacc=0.9025\t1235.2 examples/second\n",
      "Test Loss=0.4083, Test acc=0.8690\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.2847\tacc=0.8982\t494.2 examples/second\n",
      "[Step=3200]\tLoss=0.2823\tacc=0.8987\t1263.8 examples/second\n",
      "[Step=3250]\tLoss=0.2756\tacc=0.9021\t1247.9 examples/second\n",
      "[Step=3300]\tLoss=0.2779\tacc=0.9025\t1224.4 examples/second\n",
      "Test Loss=0.3974, Test acc=0.8679\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.2668\tacc=0.9021\t455.8 examples/second\n",
      "[Step=3400]\tLoss=0.2795\tacc=0.9018\t1233.0 examples/second\n",
      "[Step=3450]\tLoss=0.2772\tacc=0.9026\t1178.0 examples/second\n",
      "[Step=3500]\tLoss=0.2768\tacc=0.9035\t1219.4 examples/second\n",
      "Test Loss=0.4066, Test acc=0.8686\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.2781\tacc=0.8986\t476.7 examples/second\n",
      "[Step=3600]\tLoss=0.2611\tacc=0.9085\t1344.9 examples/second\n",
      "[Step=3650]\tLoss=0.2662\tacc=0.9071\t1187.1 examples/second\n",
      "[Step=3700]\tLoss=0.2648\tacc=0.9081\t1225.5 examples/second\n",
      "Test Loss=0.3905, Test acc=0.8731\n",
      "Saving...\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.2512\tacc=0.9112\t471.5 examples/second\n",
      "[Step=3800]\tLoss=0.2665\tacc=0.9053\t1250.9 examples/second\n",
      "[Step=3850]\tLoss=0.2643\tacc=0.9054\t1079.5 examples/second\n",
      "[Step=3900]\tLoss=0.2662\tacc=0.9055\t1202.5 examples/second\n",
      "Test Loss=0.3952, Test acc=0.8735\n",
      "Saving...\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3952, Test accuracy=0.8735\n",
      "-------------------- Nbits =  4 --------------------------------------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.4206, Test accuracy=0.8561\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.9646\tacc=0.7392\t1297.8 examples/second\n",
      "[Step=100]\tLoss=0.8368\tacc=0.7766\t1274.2 examples/second\n",
      "[Step=150]\tLoss=0.7178\tacc=0.8011\t1190.0 examples/second\n",
      "Test Loss=0.4854, Test acc=0.8339\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.3901\tacc=0.8750\t468.3 examples/second\n",
      "[Step=250]\tLoss=0.3867\tacc=0.8734\t1252.5 examples/second\n",
      "[Step=300]\tLoss=0.3743\tacc=0.8761\t1206.9 examples/second\n",
      "[Step=350]\tLoss=0.3652\tacc=0.8794\t1218.6 examples/second\n",
      "Test Loss=0.4309, Test acc=0.8563\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.3071\tacc=0.9043\t487.6 examples/second\n",
      "[Step=450]\tLoss=0.3169\tacc=0.8956\t1241.1 examples/second\n",
      "[Step=500]\tLoss=0.3091\tacc=0.8967\t1132.8 examples/second\n",
      "[Step=550]\tLoss=0.3032\tacc=0.8979\t1129.8 examples/second\n",
      "Test Loss=0.4074, Test acc=0.8653\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.2864\tacc=0.9004\t439.4 examples/second\n",
      "[Step=650]\tLoss=0.2783\tacc=0.9029\t1130.8 examples/second\n",
      "[Step=700]\tLoss=0.2770\tacc=0.9039\t1189.7 examples/second\n",
      "[Step=750]\tLoss=0.2757\tacc=0.9046\t1175.3 examples/second\n",
      "Test Loss=0.3885, Test acc=0.8716\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.2777\tacc=0.9023\t464.3 examples/second\n",
      "[Step=850]\tLoss=0.2594\tacc=0.9093\t1246.3 examples/second\n",
      "[Step=900]\tLoss=0.2607\tacc=0.9087\t1184.5 examples/second\n",
      "[Step=950]\tLoss=0.2593\tacc=0.9094\t1151.3 examples/second\n",
      "Test Loss=0.3835, Test acc=0.8756\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.2391\tacc=0.9187\t439.0 examples/second\n",
      "[Step=1050]\tLoss=0.2431\tacc=0.9164\t1234.2 examples/second\n",
      "[Step=1100]\tLoss=0.2455\tacc=0.9156\t1122.3 examples/second\n",
      "[Step=1150]\tLoss=0.2431\tacc=0.9156\t1102.2 examples/second\n",
      "Test Loss=0.3648, Test acc=0.8796\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.2358\tacc=0.9191\t484.5 examples/second\n",
      "[Step=1250]\tLoss=0.2292\tacc=0.9210\t1158.1 examples/second\n",
      "[Step=1300]\tLoss=0.2258\tacc=0.9209\t1191.4 examples/second\n",
      "[Step=1350]\tLoss=0.2263\tacc=0.9212\t1195.4 examples/second\n",
      "Test Loss=0.3679, Test acc=0.8816\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.2261\tacc=0.9205\t464.8 examples/second\n",
      "[Step=1450]\tLoss=0.2208\tacc=0.9236\t1246.7 examples/second\n",
      "[Step=1500]\tLoss=0.2189\tacc=0.9231\t1131.1 examples/second\n",
      "[Step=1550]\tLoss=0.2188\tacc=0.9225\t1080.7 examples/second\n",
      "Test Loss=0.3565, Test acc=0.8850\n",
      "Saving...\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.2113\tacc=0.9237\t468.1 examples/second\n",
      "[Step=1650]\tLoss=0.2151\tacc=0.9228\t1079.9 examples/second\n",
      "[Step=1700]\tLoss=0.2146\tacc=0.9234\t1085.5 examples/second\n",
      "[Step=1750]\tLoss=0.2108\tacc=0.9251\t1167.4 examples/second\n",
      "Test Loss=0.3451, Test acc=0.8874\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.2108\tacc=0.9281\t454.9 examples/second\n",
      "[Step=1850]\tLoss=0.2037\tacc=0.9296\t1112.4 examples/second\n",
      "[Step=1900]\tLoss=0.2047\tacc=0.9289\t1126.7 examples/second\n",
      "[Step=1950]\tLoss=0.2038\tacc=0.9282\t1175.5 examples/second\n",
      "Test Loss=0.3463, Test acc=0.8892\n",
      "Saving...\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.1889\tacc=0.9342\t459.4 examples/second\n",
      "[Step=2050]\tLoss=0.1962\tacc=0.9317\t1183.1 examples/second\n",
      "[Step=2100]\tLoss=0.1961\tacc=0.9319\t1045.5 examples/second\n",
      "[Step=2150]\tLoss=0.1940\tacc=0.9330\t1241.3 examples/second\n",
      "Test Loss=0.3425, Test acc=0.8914\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.1842\tacc=0.9347\t444.9 examples/second\n",
      "[Step=2250]\tLoss=0.1892\tacc=0.9327\t1030.2 examples/second\n",
      "[Step=2300]\tLoss=0.1884\tacc=0.9328\t1084.1 examples/second\n",
      "[Step=2350]\tLoss=0.1883\tacc=0.9329\t1145.5 examples/second\n",
      "Test Loss=0.3487, Test acc=0.8920\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.1782\tacc=0.9346\t454.1 examples/second\n",
      "[Step=2450]\tLoss=0.1831\tacc=0.9344\t1067.4 examples/second\n",
      "[Step=2500]\tLoss=0.1807\tacc=0.9362\t1008.1 examples/second\n",
      "Test Loss=0.3411, Test acc=0.8919\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.1588\tacc=0.9453\t450.7 examples/second\n",
      "[Step=2600]\tLoss=0.1816\tacc=0.9369\t1062.2 examples/second\n",
      "[Step=2650]\tLoss=0.1833\tacc=0.9356\t1146.3 examples/second\n",
      "[Step=2700]\tLoss=0.1815\tacc=0.9363\t955.4 examples/second\n",
      "Test Loss=0.3308, Test acc=0.8937\n",
      "Saving...\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.1685\tacc=0.9401\t431.3 examples/second\n",
      "[Step=2800]\tLoss=0.1696\tacc=0.9397\t1164.8 examples/second\n",
      "[Step=2850]\tLoss=0.1719\tacc=0.9391\t1068.1 examples/second\n",
      "[Step=2900]\tLoss=0.1727\tacc=0.9391\t1117.2 examples/second\n",
      "Test Loss=0.3446, Test acc=0.8944\n",
      "Saving...\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.1616\tacc=0.9441\t438.5 examples/second\n",
      "[Step=3000]\tLoss=0.1664\tacc=0.9414\t1163.5 examples/second\n",
      "[Step=3050]\tLoss=0.1677\tacc=0.9413\t1072.6 examples/second\n",
      "[Step=3100]\tLoss=0.1652\tacc=0.9423\t1067.4 examples/second\n",
      "Test Loss=0.3307, Test acc=0.8967\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.1679\tacc=0.9386\t454.1 examples/second\n",
      "[Step=3200]\tLoss=0.1660\tacc=0.9408\t1035.9 examples/second\n",
      "[Step=3250]\tLoss=0.1648\tacc=0.9413\t1038.5 examples/second\n",
      "[Step=3300]\tLoss=0.1685\tacc=0.9399\t1162.2 examples/second\n",
      "Test Loss=0.3375, Test acc=0.8969\n",
      "Saving...\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.1563\tacc=0.9464\t480.1 examples/second\n",
      "[Step=3400]\tLoss=0.1666\tacc=0.9431\t1115.1 examples/second\n",
      "[Step=3450]\tLoss=0.1654\tacc=0.9427\t1111.7 examples/second\n",
      "[Step=3500]\tLoss=0.1659\tacc=0.9421\t1188.1 examples/second\n",
      "Test Loss=0.3510, Test acc=0.8944\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.1470\tacc=0.9513\t476.3 examples/second\n",
      "[Step=3600]\tLoss=0.1527\tacc=0.9463\t1195.6 examples/second\n",
      "[Step=3650]\tLoss=0.1575\tacc=0.9449\t1068.8 examples/second\n",
      "[Step=3700]\tLoss=0.1589\tacc=0.9445\t1169.9 examples/second\n",
      "Test Loss=0.3308, Test acc=0.8973\n",
      "Saving...\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.1538\tacc=0.9468\t456.8 examples/second\n",
      "[Step=3800]\tLoss=0.1516\tacc=0.9475\t1174.8 examples/second\n",
      "[Step=3850]\tLoss=0.1548\tacc=0.9461\t1132.0 examples/second\n",
      "[Step=3900]\tLoss=0.1556\tacc=0.9455\t1261.2 examples/second\n",
      "Test Loss=0.3379, Test acc=0.8971\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3308, Test accuracy=0.8973\n"
     ]
    }
   ],
   "source": [
    "# FP_layers have been changed\n",
    "for Nbits in range(2,5):\n",
    "    print(\"-\"*20,\"Nbits = \",Nbits,\"-\"*80)\n",
    "\n",
    "\n",
    "    net = ResNetCIFAR(num_layers=20, Nbits=Nbits)\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(\"net_after_global_iterative_prune.pt\"))\n",
    "    test(net)\n",
    "    # Quantized model finetuning\n",
    "    finetune(net, epochs=20, batch_size=256, lr=0.002, reg=1e-4)\n",
    "\n",
    "    # Load the model with best accuracy\n",
    "    net.load_state_dict(torch.load(\"quantized_net_after_finetune.pt\"))\n",
    "    test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab3 (e) Prune quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0.0085436649620533\n",
      "[Step=50]\tLoss=0.0697\tacc=0.9747\t715.1 examples/second\n",
      "[Step=100]\tLoss=0.0684\tacc=0.9755\t898.4 examples/second\n",
      "[Step=150]\tLoss=0.0697\tacc=0.9753\t922.9 examples/second\n",
      "Test Loss=0.3410, Test acc=0.9088\n",
      "\n",
      "Epoch: 1\n",
      "0.017190853133797646\n",
      "[Step=50]\tLoss=0.0667\tacc=0.9771\t911.7 examples/second\n",
      "[Step=100]\tLoss=0.0700\tacc=0.9756\t829.2 examples/second\n",
      "[Step=150]\tLoss=0.0691\tacc=0.9761\t820.4 examples/second\n",
      "Test Loss=0.3478, Test acc=0.9070\n",
      "\n",
      "Epoch: 2\n",
      "0.026171804219484326\n",
      "[Step=50]\tLoss=0.0737\tacc=0.9742\t851.1 examples/second\n",
      "[Step=100]\tLoss=0.0720\tacc=0.9749\t860.0 examples/second\n",
      "[Step=150]\tLoss=0.0715\tacc=0.9754\t905.3 examples/second\n",
      "Test Loss=0.3462, Test acc=0.9070\n",
      "\n",
      "Epoch: 3\n",
      "0.03563041388988495\n",
      "[Step=50]\tLoss=0.0753\tacc=0.9738\t834.6 examples/second\n",
      "[Step=100]\tLoss=0.0741\tacc=0.9736\t861.0 examples/second\n",
      "[Step=150]\tLoss=0.0744\tacc=0.9732\t891.0 examples/second\n",
      "Test Loss=0.3461, Test acc=0.9057\n",
      "\n",
      "Epoch: 4\n",
      "0.045436009764671326\n",
      "[Step=50]\tLoss=0.0848\tacc=0.9718\t919.2 examples/second\n",
      "[Step=100]\tLoss=0.0834\tacc=0.9717\t908.0 examples/second\n",
      "[Step=150]\tLoss=0.0819\tacc=0.9722\t905.5 examples/second\n",
      "Test Loss=0.3487, Test acc=0.9041\n",
      "\n",
      "Epoch: 5\n",
      "0.05624669939279555\n",
      "[Step=50]\tLoss=0.0971\tacc=0.9670\t861.0 examples/second\n",
      "[Step=100]\tLoss=0.0976\tacc=0.9666\t891.6 examples/second\n",
      "[Step=150]\tLoss=0.0970\tacc=0.9671\t886.3 examples/second\n",
      "Test Loss=0.3565, Test acc=0.9003\n",
      "\n",
      "Epoch: 6\n",
      "0.0684414654970169\n",
      "[Step=50]\tLoss=0.1236\tacc=0.9552\t911.3 examples/second\n",
      "[Step=100]\tLoss=0.1252\tacc=0.9554\t862.0 examples/second\n",
      "[Step=150]\tLoss=0.1281\tacc=0.9538\t865.8 examples/second\n",
      "Test Loss=0.3671, Test acc=0.8927\n",
      "\n",
      "Epoch: 7\n",
      "0.08223637342453002\n",
      "[Step=50]\tLoss=0.1845\tacc=0.9345\t914.9 examples/second\n",
      "[Step=100]\tLoss=0.1901\tacc=0.9327\t896.1 examples/second\n",
      "[Step=150]\tLoss=0.1876\tacc=0.9340\t906.3 examples/second\n",
      "Test Loss=0.3922, Test acc=0.8767\n",
      "\n",
      "Epoch: 8\n",
      "0.09860724061727522\n",
      "[Step=50]\tLoss=0.3345\tacc=0.8816\t879.0 examples/second\n",
      "[Step=100]\tLoss=0.3294\tacc=0.8841\t948.7 examples/second\n",
      "[Step=150]\tLoss=0.3339\tacc=0.8834\t937.4 examples/second\n",
      "Test Loss=0.4853, Test acc=0.8407\n",
      "\n",
      "Epoch: 9\n",
      "0.11971458047628403\n",
      "[Step=50]\tLoss=0.6810\tacc=0.7680\t866.3 examples/second\n",
      "[Step=100]\tLoss=0.6809\tacc=0.7676\t917.4 examples/second\n",
      "[Step=150]\tLoss=0.6814\tacc=0.7676\t911.6 examples/second\n",
      "Test Loss=0.7808, Test acc=0.7454\n",
      "\n",
      "Epoch: 10\n",
      "[Step=50]\tLoss=0.6835\tacc=0.7657\t873.9 examples/second\n",
      "[Step=100]\tLoss=0.6898\tacc=0.7629\t882.8 examples/second\n",
      "[Step=150]\tLoss=0.6853\tacc=0.7631\t916.9 examples/second\n",
      "Test Loss=0.7879, Test acc=0.7445\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=50]\tLoss=0.6853\tacc=0.7632\t839.3 examples/second\n",
      "[Step=100]\tLoss=0.6831\tacc=0.7665\t861.5 examples/second\n",
      "[Step=150]\tLoss=0.6856\tacc=0.7642\t877.3 examples/second\n",
      "Test Loss=0.7875, Test acc=0.7436\n",
      "\n",
      "Epoch: 12\n",
      "[Step=50]\tLoss=0.6755\tacc=0.7680\t804.1 examples/second\n",
      "[Step=100]\tLoss=0.6845\tacc=0.7659\t831.9 examples/second\n",
      "[Step=150]\tLoss=0.6833\tacc=0.7659\t853.7 examples/second\n",
      "Test Loss=0.7887, Test acc=0.7445\n",
      "\n",
      "Epoch: 13\n",
      "[Step=50]\tLoss=0.6864\tacc=0.7666\t853.1 examples/second\n",
      "[Step=100]\tLoss=0.6797\tacc=0.7650\t821.9 examples/second\n",
      "[Step=150]\tLoss=0.6797\tacc=0.7659\t896.1 examples/second\n",
      "Test Loss=0.7855, Test acc=0.7435\n",
      "\n",
      "Epoch: 14\n",
      "[Step=50]\tLoss=0.6741\tacc=0.7720\t859.1 examples/second\n",
      "[Step=100]\tLoss=0.6773\tacc=0.7687\t857.5 examples/second\n",
      "[Step=150]\tLoss=0.6825\tacc=0.7664\t839.3 examples/second\n",
      "Test Loss=0.7815, Test acc=0.7445\n",
      "\n",
      "Epoch: 15\n",
      "[Step=50]\tLoss=0.6850\tacc=0.7635\t791.2 examples/second\n",
      "[Step=100]\tLoss=0.6867\tacc=0.7628\t790.2 examples/second\n",
      "[Step=150]\tLoss=0.6872\tacc=0.7642\t737.9 examples/second\n",
      "Test Loss=0.7892, Test acc=0.7428\n",
      "\n",
      "Epoch: 16\n",
      "[Step=50]\tLoss=0.6938\tacc=0.7599\t879.7 examples/second\n",
      "[Step=100]\tLoss=0.6871\tacc=0.7614\t802.7 examples/second\n",
      "[Step=150]\tLoss=0.6807\tacc=0.7650\t803.6 examples/second\n",
      "Test Loss=0.7852, Test acc=0.7434\n",
      "\n",
      "Epoch: 17\n",
      "[Step=50]\tLoss=0.6875\tacc=0.7641\t843.2 examples/second\n",
      "[Step=100]\tLoss=0.6844\tacc=0.7661\t791.5 examples/second\n",
      "[Step=150]\tLoss=0.6829\tacc=0.7656\t705.6 examples/second\n",
      "Test Loss=0.7857, Test acc=0.7438\n",
      "\n",
      "Epoch: 18\n",
      "[Step=50]\tLoss=0.6785\tacc=0.7662\t752.6 examples/second\n",
      "[Step=100]\tLoss=0.6842\tacc=0.7627\t783.8 examples/second\n",
      "[Step=150]\tLoss=0.6821\tacc=0.7642\t760.0 examples/second\n",
      "Test Loss=0.7832, Test acc=0.7448\n",
      "Saving...\n",
      "\n",
      "Epoch: 19\n",
      "[Step=50]\tLoss=0.7021\tacc=0.7614\t797.6 examples/second\n",
      "[Step=100]\tLoss=0.6950\tacc=0.7623\t812.2 examples/second\n",
      "[Step=150]\tLoss=0.6882\tacc=0.7655\t736.3 examples/second\n",
      "Test Loss=0.7853, Test acc=0.7445\n"
     ]
    }
   ],
   "source": [
    "net = ResNetCIFAR(num_layers=20, Nbits=4)\n",
    "net = net.to(device)\n",
    "\n",
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "best_acc = 0.\n",
    "for epoch in range(20):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    q=(epoch+1)*8\n",
    "    \n",
    "    net.train()\n",
    "    # Increase model sparsity\n",
    "    if epoch<10:\n",
    "        global_prune_by_percentage(net, q=q)\n",
    "    if epoch<9:\n",
    "        finetune_after_prune(net, trainloader, criterion, optimizer,prune=False)\n",
    "    else:\n",
    "        finetune_after_prune(net, trainloader, criterion, optimizer)\n",
    "    \n",
    "    #Start the testing code.\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    num_val_steps = len(testloader)\n",
    "    val_acc = correct / total\n",
    "    print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n",
    "    \n",
    "    if epoch>=10:\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(\"Saving...\")\n",
    "            torch.save(net.state_dict(), \"quantized_net_after_global_iterative_prune.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of head_conv.0.conv: 0.30787037037037035\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.6575520833333334\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.6397569444444444\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.6280381944444444\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.6488715277777778\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.6315104166666666\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.6684027777777778\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.6252170138888888\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.6882595486111112\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.7261284722222222\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.7820095486111112\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.7249348958333334\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.8132595486111112\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.7331814236111112\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.7640245225694444\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.7770453559027778\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.8261176215277778\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.8527018229166666\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.9764539930555556\n",
      "Sparsity of final_fc.linear: 0.159375\n",
      "Total sparsity of: 0.7999970186631685\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.7832, Test accuracy=0.7448\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"quantized_net_after_global_iterative_prune.pt\"))\n",
    "\n",
    "zeros_sum = 0\n",
    "total_sum = 0\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        np_weight = layer.weight.data.cpu().numpy()\n",
    "        zeros = (np_weight==0).sum()\n",
    "        total = np_weight.flatten().shape[0]\n",
    "        zeros_sum+=zeros\n",
    "        total_sum+=total\n",
    "        print('Sparsity of '+name+': '+str(zeros/total))\n",
    "print('Total sparsity of: '+str(zeros_sum/total_sum))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
